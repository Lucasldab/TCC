{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import optimizer_selector\n",
    "import csv\n",
    "import dataTreatment\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "import pandas as pd\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1))\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceCNN = {\n",
    "    'Name': hp.choice('Name', ['SGD', 'RMSprop', 'Adam', 'AdamW', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']),  # Categorical hyperparameter\n",
    "    'Convoluted_Layers1': hp.quniform('Convoluted_Layers1', 16, 32, 1),  # Integer hyperparameter\n",
    "    'Convoluted_Filters1': hp.quniform('Convoluted_Filters1', 1, 3, 1),  # Integer hyperparameter\n",
    "    'Convoluted_Layers2': hp.quniform('Convoluted_Layers2', 16, 32, 1),\n",
    "    'Convoluted_Filters2': hp.quniform('Convoluted_Filters2', 1, 3, 1),\n",
    "    'Hidden_Layer1': hp.quniform('Hidden_Layer1', 32, 64, 1),\n",
    "    'Hidden_Layer2': hp.quniform('Hidden_Layer2', 16, 32, 1),\n",
    "    'Learning_Rate': hp.loguniform('Learning_Rate', 0.0001, 0.01),\n",
    "    'Batch_Size': hp.quniform('Batch_Size', 32, 128, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_CNN(params):\n",
    "    # Extract hyperparameters from the 'params' dictionary\n",
    "    name = params['Name']\n",
    "    conv_layers1 = int(params['Convoluted_Layers1'])\n",
    "    conv_filters1 = int(params['Convoluted_Filters1'])\n",
    "    conv_layers2 = int(params['Convoluted_Layers2'])\n",
    "    conv_filters2 = int(params['Convoluted_Filters2'])\n",
    "    hidden_layer1 = int(params['Hidden_Layer1'])\n",
    "    hidden_layer2 = int(params['Hidden_Layer2'])\n",
    "    learning_rate = float(params['Learning_Rate'])\n",
    "    batch_size = int(params['Batch_Size'])\n",
    "    \n",
    "     # Create a Sequential model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    model.add(layers.Conv2D(int(conv_layers1), (int(conv_filters1)), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(int(conv_layers2), (int(conv_filters2)), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Add fully connected layers\n",
    "    model.add(layers.Dense(hidden_layer1, activation='relu'))\n",
    "    model.add(layers.Dense(hidden_layer2, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model with a random optimizer\n",
    "    optimizer, name = optimizer_selector.defining_optimizer_byName(name, learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    epoc = 5\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=epoc, batch_size=batch_size)\n",
    "\n",
    "    # Extract the loss from the training history\n",
    "    history_dict = history.history\n",
    "    final_loss = history_dict['loss'][epoc-1]\n",
    "    \n",
    "    \n",
    "    return {'loss': final_loss, 'status': hyperopt.STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceMLP = {\n",
    "    'Name': hp.choice('Name', ['SGD', 'RMSprop', 'Adam', 'AdamW', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']),  # Categorical hyperparameter\n",
    "    'Hidden_Layer1': hp.quniform('Hidden_Layer1', 32, 64, 1),\n",
    "    'Hidden_Layer2': hp.quniform('Hidden_Layer2', 16, 32, 1),\n",
    "    'Learning_Rate': hp.loguniform('Learning_Rate', 0.0001, 0.01),\n",
    "    'Batch_Size': hp.quniform('Batch_Size', 32, 128, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_MLP(params):\n",
    "    # Extract hyperparameters from the 'params' dictionary\n",
    "    name = params['Name']\n",
    "    hidden_layer1 = int(params['Hidden_Layer1'])\n",
    "    hidden_layer2 = int(params['Hidden_Layer2'])\n",
    "    learning_rate = float(params['Learning_Rate'])\n",
    "    batch_size = int(params['Batch_Size'])\n",
    "    \n",
    "     # Create a Sequential model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Add fully connected layers\n",
    "    model.add(layers.Dense(hidden_layer1, activation='relu'))\n",
    "    model.add(layers.Dense(hidden_layer2, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model with a random optimizer\n",
    "    optimizer, name = optimizer_selector.defining_optimizer_byName(name, learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    epoc = 5\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=epoc, batch_size=batch_size)\n",
    "\n",
    "    # Extract the loss from the training history\n",
    "    history_dict = history.history\n",
    "    final_loss = history_dict['loss'][epoc-1]\n",
    "    \n",
    "    \n",
    "    return {'loss': final_loss, 'status': hyperopt.STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datasetNumber in range(startingDataset,21):\n",
    "\n",
    "    folderName = 'trainings/CNN_'+ samplingMethod\n",
    "    trainingFile = 'trainings/CNN_'+ samplingMethod +'/training_'+ str(datasetNumber) +'.csv'\n",
    "\n",
    "    print('Training: training_'+ str(datasetNumber) +'.csv')\n",
    "\n",
    "    # Create a CSV file if it doesn't exist to store the results\n",
    "    if not os.path.exists(trainingFile):\n",
    "        os.makedirs(folderName)\n",
    "        with open(trainingFile, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Convoluted_Layers1\", \"Convoluted_Filters1\", \"Convoluted_Layers2\", \"Convoluted_Filters2\", \"Hidden_Layer1\", \"Hidden_Layer2\", \"Learning_Rate\", \"Batch_Size\", \"Loss\"])\n",
    "            file.close()\n",
    "\n",
    "    with open(trainingFile, 'r', newline='') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        line = sum(1 for row in csv_reader)\n",
    "        file.close()\n",
    "\n",
    "    for training in range(line-1, trains, 1):\n",
    "\n",
    "        # Create a Trials object to keep track of the optimization process\n",
    "        trials = Trials()\n",
    "\n",
    "        # Use TPE to search for the best hyperparameters\n",
    "        best = fmin(fn=objective_function_MLP,\n",
    "                    space=spaceMLP,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=5,  # Adjust the number of evaluations as needed\n",
    "                    trials=trials)\n",
    "\n",
    "        print(\"Best hyperparameters:\", best)\n",
    "\n",
    "        best = pd.DataFrame.from_dict(best, orient='index')\n",
    "        best = best.transpose().reset_index()\n",
    "        best = best.drop('index', axis=1)\n",
    "\n",
    "        losses = [trial['result']['loss'] for trial in trials.trials]\n",
    "        final_loss = np.min(losses)\n",
    "\n",
    "        with open(trainingFile, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([best['Name'], best['Convoluted_Layers1'],best['Convoluted_Filters1'],best['Convoluted_Layers2'],best['Convoluted_Filters2'],best['Hidden_Layer1'],best['Hidden_Layer2'],best['Learning_Rate'],best['Batch_Size'],final_loss])\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = 100\n",
    "samplingMethod = 'TPE'\n",
    "startingDataset = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datasetNumber in range(startingDataset,21):\n",
    "\n",
    "    folderName = 'trainings/Fully_Connected_'+ samplingMethod\n",
    "    trainingFile = 'trainings/Fully_Connected_'+ samplingMethod +'/training_'+ str(datasetNumber) +'.csv'\n",
    "\n",
    "    print('Training: training_'+ str(datasetNumber) +'.csv')\n",
    "\n",
    "    # Create a CSV file if it doesn't exist to store the results\n",
    "    if not os.path.exists(trainingFile):\n",
    "        os.makedirs(folderName)\n",
    "        with open(trainingFile, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Hidden_Layer1\", \"Hidden_Layer2\", \"Learning_Rate\", \"Batch_Size\", \"Loss\"])\n",
    "            file.close()\n",
    "\n",
    "    with open(trainingFile, 'r', newline='') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        line = sum(1 for row in csv_reader)\n",
    "        file.close()\n",
    "\n",
    "    for training in range(line-1, trains, 1):\n",
    "\n",
    "        # Create a Trials object to keep track of the optimization process\n",
    "        trials = Trials()\n",
    "\n",
    "        # Use TPE to search for the best hyperparameters\n",
    "        best = fmin(fn=objective_function_MLP,\n",
    "                    space=spaceMLP,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=5,  # Adjust the number of evaluations as needed\n",
    "                    trials=trials)\n",
    "\n",
    "        print(\"Best hyperparameters:\", best)\n",
    "\n",
    "        best = pd.DataFrame.from_dict(best, orient='index')\n",
    "        best = best.transpose().reset_index()\n",
    "        best = best.drop('index', axis=1)\n",
    "\n",
    "        losses = [trial['result']['loss'] for trial in trials.trials]\n",
    "        final_loss = np.min(losses)\n",
    "\n",
    "        with open(trainingFile, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([best['Name'],best['Hidden_Layer1'],best['Hidden_Layer2'],best['Learning_Rate'],best['Batch_Size'],final_loss])\n",
    "            file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
