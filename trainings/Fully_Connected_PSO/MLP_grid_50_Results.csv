Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
Adagrad,55,19,0.01,54,0.2634873390197754
Adamax,64,22,0.01,122,0.07081309705972672
Adam,55,13,0.01,119,0.07934289425611496
Ftrl,55,29,0.01,103,2.301190137863159
Adamax,56,28,0.01,128,0.07406577467918396
RMSprop,54,29,0.01,117,0.08876815438270569
Ftrl,43,24,0.01,128,2.3011932373046875
Ftrl,64,23,0.01,97,2.301190137863159
RMSprop,35,32,0.01,63,0.13700546324253082
Ftrl,42,32,0.01,57,2.301189661026001
Adagrad,55,26,0.01,51,0.25414955615997314
Nadam,58,32,0.01,77,0.08503804355859756
Nadam,58,28,0.01,85,0.07775332033634186
Ftrl,64,26,0.01,128,2.3011927604675293
Ftrl,56,32,0.01,111,2.301194190979004
Ftrl,59,32,0.01,128,2.301192045211792
Ftrl,49,22,0.01,128,2.3011929988861084
Adadelta,47,32,0.01,88,1.181461215019226
Adam,49,18,0.01,54,0.10026606917381287
Adadelta,36,32,0.01,95,1.296926736831665
