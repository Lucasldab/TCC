Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
AdamW,58,30,0.01,90,0.08033622056245804
Ftrl,60,32,0.01,57,2.30118989944458
Adamax,51,21,0.01,128,0.07851201295852661
Adagrad,52,28,0.01,66,0.27354201674461365
Ftrl,64,32,0.01,89,2.3011910915374756
Adagrad,64,27,0.01,128,0.3200291395187378
Nadam,39,25,0.01,118,0.08631882071495056
Ftrl,63,25,0.01,78,2.301189422607422
Adadelta,26,29,0.01,128,1.79171621799469
Adagrad,44,19,0.01,107,0.31950438022613525
Adagrad,44,31,0.01,107,0.31061121821403503
Nadam,53,16,0.01,113,0.07371629774570465
Adamax,41,32,0.01,118,0.10065766423940659
Adam,59,20,0.01,128,0.07705956697463989
SGD,60,24,0.01,76,0.3736996054649353
Adadelta,60,24,0.01,128,1.3547563552856445
Nadam,59,20,0.01,108,0.07356619834899902
AdamW,42,27,0.01,128,0.09281161427497864
Ftrl,52,32,0.01,128,2.3011929988861084
AdamW,49,27,0.01,107,0.0864911898970604
