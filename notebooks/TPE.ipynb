{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import optimizerSelector\n",
    "import csv\n",
    "import dataTreatment\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "import pandas as pd\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1))\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceCNN = {\n",
    "    'Name': hp.choice('Name', ['SGD', 'RMSprop', 'Adam', 'AdamW', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']),  # Categorical hyperparameter\n",
    "    'Convoluted_Layers1': hp.quniform('Convoluted_Layers1', 16, 32, 1),  # Integer hyperparameter\n",
    "    'Convoluted_Filters1': hp.quniform('Convoluted_Filters1', 1, 3, 1),  # Integer hyperparameter\n",
    "    'Convoluted_Layers2': hp.quniform('Convoluted_Layers2', 16, 32, 1),\n",
    "    'Convoluted_Filters2': hp.quniform('Convoluted_Filters2', 1, 3, 1),\n",
    "    'Hidden_Layer1': hp.quniform('Hidden_Layer1', 32, 64, 1),\n",
    "    'Hidden_Layer2': hp.quniform('Hidden_Layer2', 16, 32, 1),\n",
    "    'Learning_Rate': hp.loguniform('Learning_Rate', 0.0001, 0.01),\n",
    "    'Batch_Size': hp.quniform('Batch_Size', 32, 128, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_CNN(params):\n",
    "    # Extract hyperparameters from the 'params' dictionary\n",
    "    name = params['Name']\n",
    "    conv_layers1 = int(params['Convoluted_Layers1'])\n",
    "    conv_filters1 = int(params['Convoluted_Filters1'])\n",
    "    conv_layers2 = int(params['Convoluted_Layers2'])\n",
    "    conv_filters2 = int(params['Convoluted_Filters2'])\n",
    "    hidden_layer1 = int(params['Hidden_Layer1'])\n",
    "    hidden_layer2 = int(params['Hidden_Layer2'])\n",
    "    learning_rate = float(params['Learning_Rate'])\n",
    "    batch_size = int(params['Batch_Size'])\n",
    "    \n",
    "     # Create a Sequential model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    model.add(layers.Conv2D(int(conv_layers1), (int(conv_filters1)), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(int(conv_layers2), (int(conv_filters2)), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Add fully connected layers\n",
    "    model.add(layers.Dense(hidden_layer1, activation='relu'))\n",
    "    model.add(layers.Dense(hidden_layer2, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model with a random optimizer\n",
    "    optimizer, name = optimizer_selector.defining_optimizer_byName(name, learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    epoc = 5\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=epoc, batch_size=batch_size)\n",
    "\n",
    "    # Extract the loss from the training history\n",
    "    history_dict = history.history\n",
    "    final_loss = history_dict['loss'][epoc-1]\n",
    "    \n",
    "    \n",
    "    return {'loss': final_loss, 'status': hyperopt.STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceMLP = {\n",
    "    'Name': hp.choice('Name', ['SGD', 'RMSprop', 'Adam', 'AdamW', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']),  # Categorical hyperparameter\n",
    "    'Hidden_Layer1': hp.quniform('Hidden_Layer1', 32, 64, 1),\n",
    "    'Hidden_Layer2': hp.quniform('Hidden_Layer2', 16, 32, 1),\n",
    "    'Learning_Rate': hp.loguniform('Learning_Rate', 0.0001, 0.01),\n",
    "    'Batch_Size': hp.quniform('Batch_Size', 32, 128, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_MLP(params):\n",
    "    # Extract hyperparameters from the 'params' dictionary\n",
    "    name = params['Name']\n",
    "    hidden_layer1 = int(params['Hidden_Layer1'])\n",
    "    hidden_layer2 = int(params['Hidden_Layer2'])\n",
    "    learning_rate = float(params['Learning_Rate'])\n",
    "    batch_size = int(params['Batch_Size'])\n",
    "    \n",
    "     # Create a Sequential model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Add fully connected layers\n",
    "    model.add(layers.Dense(hidden_layer1, activation='relu'))\n",
    "    model.add(layers.Dense(hidden_layer2, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model with a random optimizer\n",
    "    optimizer, name = optimizerSelector.defining_optimizer_byName(name, learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    epoc = 5\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=epoc, batch_size=batch_size)\n",
    "\n",
    "    # Extract the loss from the training history\n",
    "    history_dict = history.history\n",
    "    final_loss = history_dict['loss'][epoc-1]\n",
    "    \n",
    "    \n",
    "    return {'loss': final_loss, 'status': hyperopt.STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = 100\n",
    "samplingMethod = 'TPE'\n",
    "startingDataset = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: training_1.csv\n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5                                            \n",
      "\n",
      "  1/607 [..............................] - ETA: 8:52 - loss: 2.3087 - accuracy: 0.1010\n",
      " 18/607 [..............................] - ETA: 1s - loss: 243.4864 - accuracy: 0.0971\n",
      " 37/607 [>.............................] - ETA: 1s - loss: 119.6572 - accuracy: 0.0996\n",
      " 55/607 [=>............................] - ETA: 1s - loss: 81.2539 - accuracy: 0.1006 \n",
      " 74/607 [==>...........................] - ETA: 1s - loss: 60.9849 - accuracy: 0.0991\n",
      " 93/607 [===>..........................] - ETA: 1s - loss: 48.9970 - accuracy: 0.1037\n",
      "111/607 [====>.........................] - ETA: 1s - loss: 41.4263 - accuracy: 0.1053\n",
      "129/607 [=====>........................] - ETA: 1s - loss: 35.9693 - accuracy: 0.1037\n",
      "148/607 [======>.......................] - ETA: 1s - loss: 31.6483 - accuracy: 0.1037\n",
      "166/607 [=======>......................] - ETA: 1s - loss: 28.4676 - accuracy: 0.1032\n",
      "184/607 [========>.....................] - ETA: 1s - loss: 25.9094 - accuracy: 0.1016\n",
      "203/607 [=========>....................] - ETA: 1s - loss: 23.7026 - accuracy: 0.1007\n",
      "223/607 [==========>...................] - ETA: 1s - loss: 21.7845 - accuracy: 0.1003\n",
      "242/607 [==========>...................] - ETA: 1s - loss: 20.2560 - accuracy: 0.1016\n",
      "261/607 [===========>..................] - ETA: 0s - loss: 18.9504 - accuracy: 0.1020\n",
      "280/607 [============>.................] - ETA: 0s - loss: 17.8222 - accuracy: 0.1019\n",
      "298/607 [=============>................] - ETA: 0s - loss: 16.8859 - accuracy: 0.1017\n",
      "316/607 [==============>...............] - ETA: 0s - loss: 16.0556 - accuracy: 0.1020\n",
      "333/607 [===============>..............] - ETA: 0s - loss: 15.3544 - accuracy: 0.1018\n",
      "351/607 [================>.............] - ETA: 0s - loss: 14.6858 - accuracy: 0.1025\n",
      "369/607 [=================>............] - ETA: 0s - loss: 14.0828 - accuracy: 0.1025\n",
      "386/607 [==================>...........] - ETA: 0s - loss: 13.5647 - accuracy: 0.1023\n",
      "405/607 [===================>..........] - ETA: 0s - loss: 13.0371 - accuracy: 0.1021\n",
      "423/607 [===================>..........] - ETA: 0s - loss: 12.5812 - accuracy: 0.1018\n",
      "439/607 [====================>.........] - ETA: 0s - loss: 12.2068 - accuracy: 0.1011\n",
      "455/607 [=====================>........] - ETA: 0s - loss: 11.8590 - accuracy: 0.1009\n",
      "473/607 [======================>.......] - ETA: 0s - loss: 11.4960 - accuracy: 0.1011\n",
      "490/607 [=======================>......] - ETA: 0s - loss: 11.1780 - accuracy: 0.1012\n",
      "509/607 [========================>.....] - ETA: 0s - loss: 10.8475 - accuracy: 0.1012\n",
      "528/607 [=========================>....] - ETA: 0s - loss: 10.5406 - accuracy: 0.1009\n",
      "547/607 [==========================>...] - ETA: 0s - loss: 10.2550 - accuracy: 0.1005\n",
      "566/607 [==========================>...] - ETA: 0s - loss: 9.9884 - accuracy: 0.1007 \n",
      "584/607 [===========================>..] - ETA: 0s - loss: 9.7518 - accuracy: 0.1009\n",
      "602/607 [============================>.] - ETA: 0s - loss: 9.5299 - accuracy: 0.1006\n",
      "607/607 [==============================] - 3s 3ms/step - loss: 9.4817 - accuracy: 0.1006\n",
      "\n",
      "Epoch 2/5                                            \n",
      "\n",
      "  1/607 [..............................] - ETA: 3s - loss: 2.3072 - accuracy: 0.0707\n",
      " 19/607 [..............................] - ETA: 1s - loss: 2.3140 - accuracy: 0.1085\n",
      " 38/607 [>.............................] - ETA: 1s - loss: 2.3183 - accuracy: 0.1061\n",
      " 57/607 [=>............................] - ETA: 1s - loss: 2.3200 - accuracy: 0.1046\n",
      " 77/607 [==>...........................] - ETA: 1s - loss: 2.3187 - accuracy: 0.1067\n",
      " 97/607 [===>..........................] - ETA: 1s - loss: 2.3181 - accuracy: 0.1085\n",
      "116/607 [====>.........................] - ETA: 1s - loss: 2.3167 - accuracy: 0.1087\n",
      "135/607 [=====>........................] - ETA: 1s - loss: 2.3172 - accuracy: 0.1084\n",
      "153/607 [======>.......................] - ETA: 1s - loss: 2.3186 - accuracy: 0.1076\n",
      "172/607 [=======>......................] - ETA: 1s - loss: 2.3184 - accuracy: 0.1076\n",
      "191/607 [========>.....................] - ETA: 1s - loss: 2.3184 - accuracy: 0.1081\n",
      "210/607 [=========>....................] - ETA: 1s - loss: 2.3187 - accuracy: 0.1070\n",
      "230/607 [==========>...................] - ETA: 1s - loss: 2.3180 - accuracy: 0.1072\n",
      "250/607 [===========>..................] - ETA: 0s - loss: 2.3179 - accuracy: 0.1065\n",
      "269/607 [============>.................] - ETA: 0s - loss: 2.3182 - accuracy: 0.1063\n",
      "287/607 [=============>................] - ETA: 0s - loss: 2.3186 - accuracy: 0.1055\n",
      "306/607 [==============>...............] - ETA: 0s - loss: 2.3184 - accuracy: 0.1060\n",
      "325/607 [===============>..............] - ETA: 0s - loss: 2.3185 - accuracy: 0.1060\n",
      "345/607 [================>.............] - ETA: 0s - loss: 2.3186 - accuracy: 0.1060\n",
      "363/607 [================>.............] - ETA: 0s - loss: 2.3189 - accuracy: 0.1057\n",
      "379/607 [=================>............] - ETA: 0s - loss: 2.3187 - accuracy: 0.1057\n",
      "397/607 [==================>...........] - ETA: 0s - loss: 2.3182 - accuracy: 0.1057\n",
      "417/607 [===================>..........] - ETA: 0s - loss: 2.3181 - accuracy: 0.1052\n",
      "436/607 [====================>.........] - ETA: 0s - loss: 2.3183 - accuracy: 0.1053\n",
      "456/607 [=====================>........] - ETA: 0s - loss: 2.3183 - accuracy: 0.1052\n",
      "474/607 [======================>.......] - ETA: 0s - loss: 2.3181 - accuracy: 0.1051\n",
      "493/607 [=======================>......] - ETA: 0s - loss: 2.3179 - accuracy: 0.1051\n",
      "511/607 [========================>.....] - ETA: 0s - loss: 2.3179 - accuracy: 0.1049\n",
      "530/607 [=========================>....] - ETA: 0s - loss: 2.3181 - accuracy: 0.1049\n",
      "547/607 [==========================>...] - ETA: 0s - loss: 2.3181 - accuracy: 0.1046\n",
      "565/607 [==========================>...] - ETA: 0s - loss: 2.3185 - accuracy: 0.1044\n",
      "583/607 [===========================>..] - ETA: 0s - loss: 2.3184 - accuracy: 0.1045\n",
      "601/607 [============================>.] - ETA: 0s - loss: 2.3184 - accuracy: 0.1042\n",
      "607/607 [==============================] - 2s 3ms/step - loss: 2.3184 - accuracy: 0.1039\n",
      "\n",
      "Epoch 3/5                                            \n",
      "\n",
      "  1/607 [..............................] - ETA: 4s - loss: 2.3411 - accuracy: 0.0606\n",
      " 20/607 [..............................] - ETA: 1s - loss: 2.3259 - accuracy: 0.1020\n",
      " 37/607 [>.............................] - ETA: 1s - loss: 2.3225 - accuracy: 0.0964\n",
      " 53/607 [=>............................] - ETA: 1s - loss: 2.3209 - accuracy: 0.0999\n",
      " 71/607 [==>...........................] - ETA: 1s - loss: 2.3223 - accuracy: 0.1007\n",
      " 90/607 [===>..........................] - ETA: 1s - loss: 2.3205 - accuracy: 0.1044\n",
      "109/607 [====>.........................] - ETA: 1s - loss: 2.3190 - accuracy: 0.1042\n",
      "126/607 [=====>........................] - ETA: 1s - loss: 2.3185 - accuracy: 0.1037\n",
      "146/607 [======>.......................] - ETA: 1s - loss: 2.3190 - accuracy: 0.1030\n",
      "163/607 [=======>......................] - ETA: 1s - loss: 2.3195 - accuracy: 0.1027\n",
      "181/607 [=======>......................] - ETA: 1s - loss: 2.3189 - accuracy: 0.1031\n",
      "201/607 [========>.....................] - ETA: 1s - loss: 2.3186 - accuracy: 0.1037\n",
      "218/607 [=========>....................] - ETA: 1s - loss: 2.3187 - accuracy: 0.1031\n",
      "234/607 [==========>...................] - ETA: 1s - loss: 2.3188 - accuracy: 0.1037\n",
      "253/607 [===========>..................] - ETA: 1s - loss: 2.3186 - accuracy: 0.1042\n",
      "270/607 [============>.................] - ETA: 0s - loss: 2.3186 - accuracy: 0.1037\n",
      "289/607 [=============>................] - ETA: 0s - loss: 2.3181 - accuracy: 0.1030\n",
      "307/607 [==============>...............] - ETA: 0s - loss: 2.3187 - accuracy: 0.1019\n",
      "325/607 [===============>..............] - ETA: 0s - loss: 2.3185 - accuracy: 0.1023\n",
      "342/607 [===============>..............] - ETA: 0s - loss: 2.3181 - accuracy: 0.1023\n",
      "361/607 [================>.............] - ETA: 0s - loss: 2.3179 - accuracy: 0.1032\n",
      "379/607 [=================>............] - ETA: 0s - loss: 2.3180 - accuracy: 0.1032\n",
      "398/607 [==================>...........] - ETA: 0s - loss: 2.3180 - accuracy: 0.1029\n",
      "418/607 [===================>..........] - ETA: 0s - loss: 2.3180 - accuracy: 0.1031\n",
      "437/607 [====================>.........] - ETA: 0s - loss: 2.3184 - accuracy: 0.1029\n",
      "458/607 [=====================>........] - ETA: 0s - loss: 2.3186 - accuracy: 0.1026\n",
      "477/607 [======================>.......] - ETA: 0s - loss: 2.3190 - accuracy: 0.1027\n",
      "496/607 [=======================>......] - ETA: 0s - loss: 2.3189 - accuracy: 0.1027\n",
      "514/607 [========================>.....] - ETA: 0s - loss: 2.3192 - accuracy: 0.1025\n",
      "534/607 [=========================>....] - ETA: 0s - loss: 2.3196 - accuracy: 0.1024\n",
      "552/607 [==========================>...] - ETA: 0s - loss: 2.3199 - accuracy: 0.1021\n",
      "571/607 [===========================>..] - ETA: 0s - loss: 2.3199 - accuracy: 0.1019\n",
      "590/607 [============================>.] - ETA: 0s - loss: 2.3198 - accuracy: 0.1017\n",
      "607/607 [==============================] - ETA: 0s - loss: 2.3199 - accuracy: 0.1016\n",
      "607/607 [==============================] - 2s 3ms/step - loss: 2.3199 - accuracy: 0.1016\n",
      "\n",
      "Epoch 4/5                                            \n",
      "\n",
      "  1/607 [..............................] - ETA: 3s - loss: 2.3661 - accuracy: 0.0606\n",
      " 19/607 [..............................] - ETA: 1s - loss: 2.3289 - accuracy: 0.0999\n",
      " 39/607 [>.............................] - ETA: 1s - loss: 2.3277 - accuracy: 0.1015\n",
      " 58/607 [=>............................] - ETA: 1s - loss: 2.3262 - accuracy: 0.1012\n",
      " 73/607 [==>...........................] - ETA: 1s - loss: 2.3254 - accuracy: 0.0993\n",
      " 89/607 [===>..........................] - ETA: 1s - loss: 2.3238 - accuracy: 0.1015\n",
      "105/607 [====>.........................] - ETA: 1s - loss: 2.3249 - accuracy: 0.1007\n",
      "123/607 [=====>........................] - ETA: 1s - loss: 2.3239 - accuracy: 0.1010\n",
      "140/607 [=====>........................] - ETA: 1s - loss: 2.3251 - accuracy: 0.1009\n",
      "155/607 [======>.......................] - ETA: 1s - loss: 2.3252 - accuracy: 0.1014\n",
      "172/607 [=======>......................] - ETA: 1s - loss: 2.3251 - accuracy: 0.1011\n",
      "190/607 [========>.....................] - ETA: 1s - loss: 2.3247 - accuracy: 0.1013\n",
      "208/607 [=========>....................] - ETA: 1s - loss: 2.3246 - accuracy: 0.1020\n",
      "225/607 [==========>...................] - ETA: 1s - loss: 2.3243 - accuracy: 0.1019\n",
      "243/607 [===========>..................] - ETA: 1s - loss: 2.3238 - accuracy: 0.1018\n",
      "261/607 [===========>..................] - ETA: 1s - loss: 2.3230 - accuracy: 0.1022\n",
      "280/607 [============>.................] - ETA: 0s - loss: 2.3232 - accuracy: 0.1022\n",
      "300/607 [=============>................] - ETA: 0s - loss: 2.3233 - accuracy: 0.1019\n",
      "319/607 [==============>...............] - ETA: 0s - loss: 2.3237 - accuracy: 0.1018\n",
      "337/607 [===============>..............] - ETA: 0s - loss: 2.3239 - accuracy: 0.1018\n",
      "356/607 [================>.............] - ETA: 0s - loss: 2.3241 - accuracy: 0.1014\n",
      "376/607 [=================>............] - ETA: 0s - loss: 2.3241 - accuracy: 0.1015\n",
      "394/607 [==================>...........] - ETA: 0s - loss: 2.3239 - accuracy: 0.1012\n",
      "413/607 [===================>..........] - ETA: 0s - loss: 2.3234 - accuracy: 0.1016\n",
      "432/607 [====================>.........] - ETA: 0s - loss: 2.3230 - accuracy: 0.1020\n",
      "448/607 [=====================>........] - ETA: 0s - loss: 2.3228 - accuracy: 0.1022\n",
      "464/607 [=====================>........] - ETA: 0s - loss: 2.3227 - accuracy: 0.1024\n",
      "480/607 [======================>.......] - ETA: 0s - loss: 2.3225 - accuracy: 0.1026\n",
      "495/607 [=======================>......] - ETA: 0s - loss: 2.3225 - accuracy: 0.1028\n",
      "512/607 [========================>.....] - ETA: 0s - loss: 2.3222 - accuracy: 0.1028\n",
      "533/607 [=========================>....] - ETA: 0s - loss: 2.3223 - accuracy: 0.1021\n",
      "550/607 [==========================>...] - ETA: 0s - loss: 2.3221 - accuracy: 0.1023\n",
      "567/607 [===========================>..] - ETA: 0s - loss: 2.3224 - accuracy: 0.1019\n",
      "584/607 [===========================>..] - ETA: 0s - loss: 2.3226 - accuracy: 0.1020\n",
      "601/607 [============================>.] - ETA: 0s - loss: 2.3233 - accuracy: 0.1015\n",
      "607/607 [==============================] - 2s 3ms/step - loss: 2.3233 - accuracy: 0.1015\n",
      "\n",
      "Epoch 5/5                                            \n",
      "\n",
      "  1/607 [..............................] - ETA: 4s - loss: 2.3068 - accuracy: 0.0808\n",
      " 13/607 [..............................] - ETA: 2s - loss: 2.3363 - accuracy: 0.0925\n",
      " 25/607 [>.............................] - ETA: 2s - loss: 2.3248 - accuracy: 0.1103\n",
      " 41/607 [=>............................] - ETA: 2s - loss: 2.3223 - accuracy: 0.1094\n",
      " 55/607 [=>............................] - ETA: 2s - loss: 2.3207 - accuracy: 0.1069\n",
      " 72/607 [==>...........................] - ETA: 1s - loss: 2.3200 - accuracy: 0.1054\n",
      " 89/607 [===>..........................] - ETA: 1s - loss: 2.3204 - accuracy: 0.1044\n",
      "104/607 [====>.........................] - ETA: 1s - loss: 2.3205 - accuracy: 0.1033\n",
      "118/607 [====>.........................] - ETA: 1s - loss: 2.3215 - accuracy: 0.1032\n",
      "135/607 [=====>........................] - ETA: 1s - loss: 2.3213 - accuracy: 0.1029\n",
      "153/607 [======>.......................] - ETA: 1s - loss: 2.3215 - accuracy: 0.1024\n",
      "171/607 [=======>......................] - ETA: 1s - loss: 2.3219 - accuracy: 0.1015\n",
      "189/607 [========>.....................] - ETA: 1s - loss: 2.3211 - accuracy: 0.1002\n",
      "207/607 [=========>....................] - ETA: 1s - loss: 2.3204 - accuracy: 0.1018\n",
      "224/607 [==========>...................] - ETA: 1s - loss: 2.3218 - accuracy: 0.1007\n",
      "240/607 [==========>...................] - ETA: 1s - loss: 2.3221 - accuracy: 0.1011\n",
      "256/607 [===========>..................] - ETA: 1s - loss: 2.3218 - accuracy: 0.1009\n",
      "272/607 [============>.................] - ETA: 1s - loss: 2.3217 - accuracy: 0.1012\n",
      "286/607 [=============>................] - ETA: 1s - loss: 2.3219 - accuracy: 0.1012\n",
      "302/607 [=============>................] - ETA: 0s - loss: 2.3220 - accuracy: 0.1017\n",
      "320/607 [==============>...............] - ETA: 0s - loss: 2.3215 - accuracy: 0.1029\n",
      "339/607 [===============>..............] - ETA: 0s - loss: 2.3216 - accuracy: 0.1036\n",
      "357/607 [================>.............] - ETA: 0s - loss: 2.3215 - accuracy: 0.1036\n",
      "376/607 [=================>............] - ETA: 0s - loss: 2.3222 - accuracy: 0.1033\n",
      "394/607 [==================>...........] - ETA: 0s - loss: 2.3230 - accuracy: 0.1031\n",
      "412/607 [===================>..........] - ETA: 0s - loss: 2.3232 - accuracy: 0.1032\n",
      "429/607 [====================>.........] - ETA: 0s - loss: 2.3236 - accuracy: 0.1032\n",
      "448/607 [=====================>........] - ETA: 0s - loss: 2.3238 - accuracy: 0.1025\n",
      "466/607 [======================>.......] - ETA: 0s - loss: 2.3235 - accuracy: 0.1025\n",
      "485/607 [======================>.......] - ETA: 0s - loss: 2.3235 - accuracy: 0.1026\n",
      "504/607 [=======================>......] - ETA: 0s - loss: 2.3236 - accuracy: 0.1026\n",
      "524/607 [========================>.....] - ETA: 0s - loss: 2.3237 - accuracy: 0.1025\n",
      "542/607 [=========================>....] - ETA: 0s - loss: 2.3233 - accuracy: 0.1027\n",
      "560/607 [==========================>...] - ETA: 0s - loss: 2.3233 - accuracy: 0.1030\n",
      "580/607 [===========================>..] - ETA: 0s - loss: 2.3238 - accuracy: 0.1024\n",
      "599/607 [============================>.] - ETA: 0s - loss: 2.3239 - accuracy: 0.1023\n",
      "607/607 [==============================] - 2s 3ms/step - loss: 2.3238 - accuracy: 0.1024\n",
      "\n",
      "Epoch 1/5                                                                      \n",
      "\n",
      "  1/760 [..............................] - ETA: 13:28 - loss: 2.2636 - accuracy: 0.2152\n",
      " 18/760 [..............................] - ETA: 2s - loss: 566.9911 - accuracy: 0.1118 \n",
      " 35/760 [>.............................] - ETA: 2s - loss: 292.8188 - accuracy: 0.1154\n",
      " 51/760 [=>............................] - ETA: 2s - loss: 201.6958 - accuracy: 0.1132\n",
      " 67/760 [=>............................] - ETA: 2s - loss: 154.0860 - accuracy: 0.1109\n",
      " 84/760 [==>...........................] - ETA: 2s - loss: 123.3730 - accuracy: 0.1062\n",
      " 97/760 [==>...........................] - ETA: 2s - loss: 107.1505 - accuracy: 0.1053\n",
      "113/760 [===>..........................] - ETA: 2s - loss: 92.3133 - accuracy: 0.1034 \n",
      "130/760 [====>.........................] - ETA: 2s - loss: 80.5464 - accuracy: 0.1041\n",
      "147/760 [====>.........................] - ETA: 1s - loss: 71.5012 - accuracy: 0.1041\n",
      "165/760 [=====>........................] - ETA: 1s - loss: 63.9550 - accuracy: 0.1033\n",
      "182/760 [======>.......................] - ETA: 1s - loss: 58.2017 - accuracy: 0.1034\n",
      "199/760 [======>.......................] - ETA: 1s - loss: 53.4293 - accuracy: 0.1041\n",
      "217/760 [=======>......................] - ETA: 1s - loss: 49.1931 - accuracy: 0.1040\n",
      "233/760 [========>.....................] - ETA: 1s - loss: 45.9782 - accuracy: 0.1036\n",
      "250/760 [========>.....................] - ETA: 1s - loss: 43.0105 - accuracy: 0.1031\n",
      "267/760 [=========>....................] - ETA: 1s - loss: 40.4219 - accuracy: 0.1033\n",
      "284/760 [==========>...................] - ETA: 1s - loss: 38.1447 - accuracy: 0.1027\n",
      "299/760 [==========>...................] - ETA: 1s - loss: 36.3486 - accuracy: 0.1029\n",
      "315/760 [===========>..................] - ETA: 1s - loss: 34.6210 - accuracy: 0.1028\n",
      "332/760 [============>.................] - ETA: 1s - loss: 32.9686 - accuracy: 0.1036\n",
      "350/760 [============>.................] - ETA: 1s - loss: 31.3949 - accuracy: 0.1037\n",
      "367/760 [=============>................] - ETA: 1s - loss: 30.0511 - accuracy: 0.1034\n",
      "385/760 [==============>...............] - ETA: 1s - loss: 28.7563 - accuracy: 0.1030\n",
      "404/760 [==============>...............] - ETA: 1s - loss: 27.5141 - accuracy: 0.1033\n",
      "420/760 [===============>..............] - ETA: 1s - loss: 26.5552 - accuracy: 0.1036\n",
      "436/760 [================>.............] - ETA: 0s - loss: 25.6668 - accuracy: 0.1042\n",
      "452/760 [================>.............] - ETA: 0s - loss: 24.8422 - accuracy: 0.1041\n",
      "467/760 [=================>............] - ETA: 0s - loss: 24.1192 - accuracy: 0.1043\n",
      "482/760 [==================>...........] - ETA: 0s - loss: 23.4417 - accuracy: 0.1043\n",
      "498/760 [==================>...........] - ETA: 0s - loss: 22.7650 - accuracy: 0.1044\n",
      "514/760 [===================>..........] - ETA: 0s - loss: 22.1293 - accuracy: 0.1043\n",
      "526/760 [===================>..........] - ETA: 0s - loss: 21.6782 - accuracy: 0.1042\n",
      "539/760 [====================>.........] - ETA: 0s - loss: 21.2122 - accuracy: 0.1041\n",
      "552/760 [====================>.........] - ETA: 0s - loss: 20.7680 - accuracy: 0.1042\n",
      "565/760 [=====================>........] - ETA: 0s - loss: 20.3440 - accuracy: 0.1045\n",
      "578/760 [=====================>........] - ETA: 0s - loss: 19.9395 - accuracy: 0.1046\n",
      "592/760 [======================>.......] - ETA: 0s - loss: 19.5237 - accuracy: 0.1048\n",
      "607/760 [======================>.......] - ETA: 0s - loss: 19.0988 - accuracy: 0.1047\n",
      "623/760 [=======================>......] - ETA: 0s - loss: 18.6686 - accuracy: 0.1048\n",
      "638/760 [========================>.....] - ETA: 0s - loss: 18.2850 - accuracy: 0.1046\n",
      "653/760 [========================>.....] - ETA: 0s - loss: 17.9189 - accuracy: 0.1047\n",
      "664/760 [=========================>....] - ETA: 0s - loss: 17.6609 - accuracy: 0.1048\n",
      "677/760 [=========================>....] - ETA: 0s - loss: 17.3667 - accuracy: 0.1046\n",
      "693/760 [==========================>...] - ETA: 0s - loss: 17.0200 - accuracy: 0.1046\n",
      "710/760 [===========================>..] - ETA: 0s - loss: 16.6684 - accuracy: 0.1047\n",
      "727/760 [===========================>..] - ETA: 0s - loss: 16.3338 - accuracy: 0.1046\n",
      "743/760 [============================>.] - ETA: 0s - loss: 16.0321 - accuracy: 0.1046\n",
      "758/760 [============================>.] - ETA: 0s - loss: 15.7613 - accuracy: 0.1048\n",
      "760/760 [==============================] - 4s 3ms/step - loss: 15.7349 - accuracy: 0.1047\n",
      "\n",
      "Epoch 2/5                                                                      \n",
      "\n",
      "  1/760 [..............................] - ETA: 6s - loss: 2.3559 - accuracy: 0.0633\n",
      " 18/760 [..............................] - ETA: 2s - loss: 2.3714 - accuracy: 0.0999\n",
      " 36/760 [>.............................] - ETA: 2s - loss: 2.3672 - accuracy: 0.1086\n",
      " 52/760 [=>............................] - ETA: 2s - loss: 2.3732 - accuracy: 0.1047\n",
      " 69/760 [=>............................] - ETA: 2s - loss: 2.3714 - accuracy: 0.1022\n",
      " 85/760 [==>...........................] - ETA: 2s - loss: 2.3687 - accuracy: 0.1026\n",
      "101/760 [==>...........................] - ETA: 2s - loss: 2.3678 - accuracy: 0.1038\n",
      "118/760 [===>..........................] - ETA: 1s - loss: 2.3678 - accuracy: 0.1028\n",
      "135/760 [====>.........................] - ETA: 1s - loss: 2.3679 - accuracy: 0.1033\n",
      "154/760 [=====>........................] - ETA: 1s - loss: 2.3665 - accuracy: 0.1027\n",
      "172/760 [=====>........................] - ETA: 1s - loss: 2.3627 - accuracy: 0.1033\n",
      "190/760 [======>.......................] - ETA: 1s - loss: 2.3604 - accuracy: 0.1033\n",
      "207/760 [=======>......................] - ETA: 1s - loss: 2.3587 - accuracy: 0.1045\n",
      "224/760 [=======>......................] - ETA: 1s - loss: 2.3607 - accuracy: 0.1037\n",
      "239/760 [========>.....................] - ETA: 1s - loss: 2.3606 - accuracy: 0.1036\n",
      "255/760 [=========>....................] - ETA: 1s - loss: 2.3618 - accuracy: 0.1034\n",
      "272/760 [=========>....................] - ETA: 1s - loss: 2.3616 - accuracy: 0.1034\n",
      "289/760 [==========>...................] - ETA: 1s - loss: 2.3606 - accuracy: 0.1031\n",
      "306/760 [===========>..................] - ETA: 1s - loss: 2.3600 - accuracy: 0.1032\n",
      "323/760 [===========>..................] - ETA: 1s - loss: 2.3596 - accuracy: 0.1024\n",
      "342/760 [============>.................] - ETA: 1s - loss: 2.3596 - accuracy: 0.1023\n",
      "357/760 [=============>................] - ETA: 1s - loss: 2.3603 - accuracy: 0.1020\n",
      "372/760 [=============>................] - ETA: 1s - loss: 2.3604 - accuracy: 0.1024\n",
      "389/760 [==============>...............] - ETA: 1s - loss: 2.3605 - accuracy: 0.1024\n",
      "406/760 [===============>..............] - ETA: 1s - loss: 2.3605 - accuracy: 0.1021\n",
      "421/760 [===============>..............] - ETA: 1s - loss: 2.3608 - accuracy: 0.1022\n",
      "439/760 [================>.............] - ETA: 0s - loss: 2.3619 - accuracy: 0.1020\n",
      "457/760 [=================>............] - ETA: 0s - loss: 2.3610 - accuracy: 0.1022\n",
      "474/760 [=================>............] - ETA: 0s - loss: 2.3606 - accuracy: 0.1021\n",
      "493/760 [==================>...........] - ETA: 0s - loss: 2.3556 - accuracy: 0.1046\n",
      "512/760 [===================>..........] - ETA: 0s - loss: 2.3461 - accuracy: 0.1086\n",
      "530/760 [===================>..........] - ETA: 0s - loss: 2.3430 - accuracy: 0.1103\n",
      "548/760 [====================>.........] - ETA: 0s - loss: 4.3164 - accuracy: 0.1110\n",
      "563/760 [=====================>........] - ETA: 0s - loss: 4.3382 - accuracy: 0.1108\n",
      "578/760 [=====================>........] - ETA: 0s - loss: 4.2957 - accuracy: 0.1108\n",
      "597/760 [======================>.......] - ETA: 0s - loss: 4.2348 - accuracy: 0.1103\n",
      "615/760 [=======================>......] - ETA: 0s - loss: 4.1799 - accuracy: 0.1098\n",
      "632/760 [=======================>......] - ETA: 0s - loss: 4.1312 - accuracy: 0.1094\n",
      "651/760 [========================>.....] - ETA: 0s - loss: 4.0797 - accuracy: 0.1088\n",
      "669/760 [=========================>....] - ETA: 0s - loss: 4.0330 - accuracy: 0.1087\n",
      "687/760 [==========================>...] - ETA: 0s - loss: 3.9886 - accuracy: 0.1085\n",
      "701/760 [==========================>...] - ETA: 0s - loss: 3.9565 - accuracy: 0.1084\n",
      "714/760 [===========================>..] - ETA: 0s - loss: 3.9277 - accuracy: 0.1085\n",
      "731/760 [===========================>..] - ETA: 0s - loss: 3.8918 - accuracy: 0.1087\n",
      "749/760 [============================>.] - ETA: 0s - loss: 3.8548 - accuracy: 0.1085\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 3.8343 - accuracy: 0.1082\n",
      "\n",
      "Epoch 3/5                                                                      \n",
      "\n",
      "  1/760 [..............................] - ETA: 6s - loss: 2.2954 - accuracy: 0.1013\n",
      " 17/760 [..............................] - ETA: 2s - loss: 2.3513 - accuracy: 0.0908\n",
      " 34/760 [>.............................] - ETA: 2s - loss: 2.3453 - accuracy: 0.1039\n",
      " 52/760 [=>............................] - ETA: 2s - loss: 2.3557 - accuracy: 0.1018\n",
      " 70/760 [=>............................] - ETA: 2s - loss: 2.3571 - accuracy: 0.0986\n",
      " 89/760 [==>...........................] - ETA: 1s - loss: 2.3555 - accuracy: 0.0996\n",
      "106/760 [===>..........................] - ETA: 1s - loss: 2.3595 - accuracy: 0.1016\n",
      "124/760 [===>..........................] - ETA: 1s - loss: 2.3602 - accuracy: 0.1035\n",
      "143/760 [====>.........................] - ETA: 1s - loss: 2.3609 - accuracy: 0.1024\n",
      "162/760 [=====>........................] - ETA: 1s - loss: 2.3602 - accuracy: 0.1026\n",
      "178/760 [======>.......................] - ETA: 1s - loss: 2.3604 - accuracy: 0.1038\n",
      "195/760 [======>.......................] - ETA: 1s - loss: 2.3609 - accuracy: 0.1032\n",
      "212/760 [=======>......................] - ETA: 1s - loss: 2.3626 - accuracy: 0.1029\n",
      "231/760 [========>.....................] - ETA: 1s - loss: 2.3627 - accuracy: 0.1023\n",
      "248/760 [========>.....................] - ETA: 1s - loss: 2.3614 - accuracy: 0.1027\n",
      "266/760 [=========>....................] - ETA: 1s - loss: 2.3600 - accuracy: 0.1025\n",
      "283/760 [==========>...................] - ETA: 1s - loss: 2.3608 - accuracy: 0.1019\n",
      "297/760 [==========>...................] - ETA: 1s - loss: 2.3607 - accuracy: 0.1021\n",
      "314/760 [===========>..................] - ETA: 1s - loss: 2.3594 - accuracy: 0.1034\n",
      "328/760 [===========>..................] - ETA: 1s - loss: 2.3594 - accuracy: 0.1031\n",
      "342/760 [============>.................] - ETA: 1s - loss: 2.3600 - accuracy: 0.1028\n",
      "353/760 [============>.................] - ETA: 1s - loss: 2.3602 - accuracy: 0.1023\n",
      "363/760 [=============>................] - ETA: 1s - loss: 2.3602 - accuracy: 0.1024\n",
      "378/760 [=============>................] - ETA: 1s - loss: 2.3602 - accuracy: 0.1022\n",
      "395/760 [==============>...............] - ETA: 1s - loss: 2.3609 - accuracy: 0.1019\n",
      "411/760 [===============>..............] - ETA: 1s - loss: 2.3600 - accuracy: 0.1021\n",
      "428/760 [===============>..............] - ETA: 1s - loss: 2.3594 - accuracy: 0.1019\n",
      "445/760 [================>.............] - ETA: 0s - loss: 2.3597 - accuracy: 0.1021\n",
      "461/760 [=================>............] - ETA: 0s - loss: 2.3605 - accuracy: 0.1017\n",
      "479/760 [=================>............] - ETA: 0s - loss: 2.3615 - accuracy: 0.1014\n",
      "497/760 [==================>...........] - ETA: 0s - loss: 2.3627 - accuracy: 0.1013\n",
      "512/760 [===================>..........] - ETA: 0s - loss: 2.3623 - accuracy: 0.1015\n",
      "528/760 [===================>..........] - ETA: 0s - loss: 2.3614 - accuracy: 0.1014\n",
      "545/760 [====================>.........] - ETA: 0s - loss: 2.3602 - accuracy: 0.1018\n",
      "566/760 [=====================>........] - ETA: 0s - loss: 2.3597 - accuracy: 0.1016\n",
      "585/760 [======================>.......] - ETA: 0s - loss: 2.3601 - accuracy: 0.1016\n",
      "603/760 [======================>.......] - ETA: 0s - loss: 2.3602 - accuracy: 0.1017\n",
      "621/760 [=======================>......] - ETA: 0s - loss: 2.3607 - accuracy: 0.1017\n",
      "640/760 [========================>.....] - ETA: 0s - loss: 2.3605 - accuracy: 0.1017\n",
      "656/760 [========================>.....] - ETA: 0s - loss: 2.3603 - accuracy: 0.1018\n",
      "672/760 [=========================>....] - ETA: 0s - loss: 2.3605 - accuracy: 0.1020\n",
      "687/760 [==========================>...] - ETA: 0s - loss: 2.3607 - accuracy: 0.1018\n",
      "703/760 [==========================>...] - ETA: 0s - loss: 2.3602 - accuracy: 0.1019\n",
      "717/760 [===========================>..] - ETA: 0s - loss: 2.3606 - accuracy: 0.1022\n",
      "733/760 [===========================>..] - ETA: 0s - loss: 2.3605 - accuracy: 0.1022\n",
      "752/760 [============================>.] - ETA: 0s - loss: 2.3607 - accuracy: 0.1020\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 2.3608 - accuracy: 0.1021\n",
      "\n",
      "Epoch 4/5                                                                      \n",
      "\n",
      "  1/760 [..............................] - ETA: 6s - loss: 2.3439 - accuracy: 0.1772\n",
      " 15/760 [..............................] - ETA: 2s - loss: 2.3668 - accuracy: 0.1021\n",
      " 30/760 [>.............................] - ETA: 2s - loss: 2.3765 - accuracy: 0.1013\n",
      " 48/760 [>.............................] - ETA: 2s - loss: 2.3679 - accuracy: 0.0973\n",
      " 65/760 [=>............................] - ETA: 2s - loss: 2.3606 - accuracy: 0.1024\n",
      " 82/760 [==>...........................] - ETA: 2s - loss: 2.3575 - accuracy: 0.1002\n",
      " 99/760 [==>...........................] - ETA: 2s - loss: 2.3571 - accuracy: 0.1042\n",
      "112/760 [===>..........................] - ETA: 2s - loss: 2.3573 - accuracy: 0.1041\n",
      "127/760 [====>.........................] - ETA: 2s - loss: 2.3587 - accuracy: 0.1039\n",
      "144/760 [====>.........................] - ETA: 1s - loss: 2.3565 - accuracy: 0.1042\n",
      "162/760 [=====>........................] - ETA: 1s - loss: 2.3542 - accuracy: 0.1046\n",
      "180/760 [======>.......................] - ETA: 1s - loss: 2.3538 - accuracy: 0.1049\n",
      "198/760 [======>.......................] - ETA: 1s - loss: 2.3540 - accuracy: 0.1063\n",
      "215/760 [=======>......................] - ETA: 1s - loss: 2.3570 - accuracy: 0.1064\n",
      "232/760 [========>.....................] - ETA: 1s - loss: 2.3582 - accuracy: 0.1044\n",
      "248/760 [========>.....................] - ETA: 1s - loss: 2.3593 - accuracy: 0.1046\n",
      "263/760 [=========>....................] - ETA: 1s - loss: 2.3604 - accuracy: 0.1044\n",
      "282/760 [==========>...................] - ETA: 1s - loss: 2.3613 - accuracy: 0.1046\n",
      "301/760 [==========>...................] - ETA: 1s - loss: 2.3626 - accuracy: 0.1045\n",
      "317/760 [===========>..................] - ETA: 1s - loss: 2.3633 - accuracy: 0.1045\n",
      "333/760 [============>.................] - ETA: 1s - loss: 2.3622 - accuracy: 0.1044\n",
      "351/760 [============>.................] - ETA: 1s - loss: 2.3621 - accuracy: 0.1040\n",
      "367/760 [=============>................] - ETA: 1s - loss: 2.3629 - accuracy: 0.1039\n",
      "383/760 [==============>...............] - ETA: 1s - loss: 2.3626 - accuracy: 0.1039\n",
      "402/760 [==============>...............] - ETA: 1s - loss: 2.3611 - accuracy: 0.1043\n",
      "421/760 [===============>..............] - ETA: 1s - loss: 2.3609 - accuracy: 0.1039\n",
      "440/760 [================>.............] - ETA: 0s - loss: 2.3605 - accuracy: 0.1033\n",
      "457/760 [=================>............] - ETA: 0s - loss: 2.3596 - accuracy: 0.1029\n",
      "475/760 [=================>............] - ETA: 0s - loss: 2.3608 - accuracy: 0.1026\n",
      "493/760 [==================>...........] - ETA: 0s - loss: 2.3610 - accuracy: 0.1027\n",
      "508/760 [===================>..........] - ETA: 0s - loss: 2.3614 - accuracy: 0.1026\n",
      "525/760 [===================>..........] - ETA: 0s - loss: 2.3614 - accuracy: 0.1024\n",
      "543/760 [====================>.........] - ETA: 0s - loss: 2.3615 - accuracy: 0.1023\n",
      "562/760 [=====================>........] - ETA: 0s - loss: 2.3620 - accuracy: 0.1019\n",
      "580/760 [=====================>........] - ETA: 0s - loss: 2.3613 - accuracy: 0.1019\n",
      "596/760 [======================>.......] - ETA: 0s - loss: 2.3607 - accuracy: 0.1020\n",
      "613/760 [=======================>......] - ETA: 0s - loss: 2.3613 - accuracy: 0.1021\n",
      "631/760 [=======================>......] - ETA: 0s - loss: 2.3609 - accuracy: 0.1019\n",
      "647/760 [========================>.....] - ETA: 0s - loss: 2.3608 - accuracy: 0.1022\n",
      "663/760 [=========================>....] - ETA: 0s - loss: 2.3607 - accuracy: 0.1024\n",
      "678/760 [=========================>....] - ETA: 0s - loss: 2.3607 - accuracy: 0.1020\n",
      "694/760 [==========================>...] - ETA: 0s - loss: 2.3604 - accuracy: 0.1018\n",
      "709/760 [==========================>...] - ETA: 0s - loss: 2.3597 - accuracy: 0.1021\n",
      "725/760 [===========================>..] - ETA: 0s - loss: 2.3596 - accuracy: 0.1019\n",
      "740/760 [============================>.] - ETA: 0s - loss: 2.3594 - accuracy: 0.1019\n",
      "755/760 [============================>.] - ETA: 0s - loss: 2.3590 - accuracy: 0.1018\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 2.3589 - accuracy: 0.1018\n",
      "\n",
      "Epoch 5/5                                                                      \n",
      "\n",
      "  1/760 [..............................] - ETA: 5s - loss: 2.4363 - accuracy: 0.1013\n",
      " 16/760 [..............................] - ETA: 2s - loss: 2.3724 - accuracy: 0.0997\n",
      " 32/760 [>.............................] - ETA: 2s - loss: 2.3658 - accuracy: 0.1021\n",
      " 48/760 [>.............................] - ETA: 2s - loss: 2.3734 - accuracy: 0.1034\n",
      " 62/760 [=>............................] - ETA: 2s - loss: 2.3756 - accuracy: 0.1017\n",
      " 76/760 [==>...........................] - ETA: 2s - loss: 2.3724 - accuracy: 0.0999\n",
      " 93/760 [==>...........................] - ETA: 2s - loss: 2.3704 - accuracy: 0.1010\n",
      "111/760 [===>..........................] - ETA: 2s - loss: 2.3657 - accuracy: 0.1025\n",
      "129/760 [====>.........................] - ETA: 1s - loss: 2.3611 - accuracy: 0.1005\n",
      "148/760 [====>.........................] - ETA: 1s - loss: 2.3585 - accuracy: 0.0999\n",
      "167/760 [=====>........................] - ETA: 1s - loss: 2.3586 - accuracy: 0.0979\n",
      "186/760 [======>.......................] - ETA: 1s - loss: 2.3573 - accuracy: 0.0973\n",
      "204/760 [=======>......................] - ETA: 1s - loss: 2.3541 - accuracy: 0.0977\n",
      "223/760 [=======>......................] - ETA: 1s - loss: 2.3520 - accuracy: 0.0989\n",
      "242/760 [========>.....................] - ETA: 1s - loss: 2.3509 - accuracy: 0.0994\n",
      "261/760 [=========>....................] - ETA: 1s - loss: 2.3526 - accuracy: 0.0987\n",
      "278/760 [=========>....................] - ETA: 1s - loss: 2.3551 - accuracy: 0.0992\n",
      "295/760 [==========>...................] - ETA: 1s - loss: 2.3559 - accuracy: 0.0993\n",
      "311/760 [===========>..................] - ETA: 1s - loss: 2.3569 - accuracy: 0.0990\n",
      "328/760 [===========>..................] - ETA: 1s - loss: 2.3563 - accuracy: 0.0993\n",
      "342/760 [============>.................] - ETA: 1s - loss: 2.3563 - accuracy: 0.0987\n",
      "358/760 [=============>................] - ETA: 1s - loss: 2.3554 - accuracy: 0.0997\n",
      "374/760 [=============>................] - ETA: 1s - loss: 2.3564 - accuracy: 0.0995\n",
      "390/760 [==============>...............] - ETA: 1s - loss: 2.3572 - accuracy: 0.0993\n",
      "405/760 [==============>...............] - ETA: 1s - loss: 2.3574 - accuracy: 0.0994\n",
      "420/760 [===============>..............] - ETA: 1s - loss: 2.3575 - accuracy: 0.0992\n",
      "434/760 [================>.............] - ETA: 0s - loss: 2.3574 - accuracy: 0.0993\n",
      "448/760 [================>.............] - ETA: 0s - loss: 2.3567 - accuracy: 0.0995\n",
      "465/760 [=================>............] - ETA: 0s - loss: 2.3561 - accuracy: 0.0997\n",
      "482/760 [==================>...........] - ETA: 0s - loss: 2.3561 - accuracy: 0.1002\n",
      "497/760 [==================>...........] - ETA: 0s - loss: 2.3564 - accuracy: 0.1005\n",
      "516/760 [===================>..........] - ETA: 0s - loss: 2.3569 - accuracy: 0.1008\n",
      "532/760 [====================>.........] - ETA: 0s - loss: 2.3574 - accuracy: 0.1006\n",
      "548/760 [====================>.........] - ETA: 0s - loss: 2.3569 - accuracy: 0.1009\n",
      "564/760 [=====================>........] - ETA: 0s - loss: 2.3561 - accuracy: 0.1008\n",
      "582/760 [=====================>........] - ETA: 0s - loss: 2.3566 - accuracy: 0.1008\n",
      "601/760 [======================>.......] - ETA: 0s - loss: 2.3574 - accuracy: 0.1005\n",
      "614/760 [=======================>......] - ETA: 0s - loss: 2.3568 - accuracy: 0.1003\n",
      "627/760 [=======================>......] - ETA: 0s - loss: 2.3561 - accuracy: 0.1003\n",
      "643/760 [========================>.....] - ETA: 0s - loss: 2.3561 - accuracy: 0.1001\n",
      "661/760 [=========================>....] - ETA: 0s - loss: 2.3573 - accuracy: 0.1002\n",
      "678/760 [=========================>....] - ETA: 0s - loss: 2.3575 - accuracy: 0.1003\n",
      "696/760 [==========================>...] - ETA: 0s - loss: 2.3581 - accuracy: 0.1000\n",
      " 20%|██        | 1/5 [00:22<01:30, 22.74s/trial, best loss: 2.3238303661346436]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projetos\\PesquisaArtigo\\TCC_Hyperparameters Optimization\\TPE.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m trials \u001b[39m=\u001b[39m Trials()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Use TPE to search for the best hyperparameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m best \u001b[39m=\u001b[39m fmin(fn\u001b[39m=\u001b[39;49mobjective_function_MLP,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             space\u001b[39m=\u001b[39;49mspaceMLP,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m             algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m             max_evals\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,  \u001b[39m# Adjust the number of evaluations as needed\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m             trials\u001b[39m=\u001b[39;49mtrials)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest hyperparameters:\u001b[39m\u001b[39m\"\u001b[39m, best)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m best \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(best, orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[0;32m    541\u001b[0m         fn,\n\u001b[0;32m    542\u001b[0m         space,\n\u001b[0;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[0;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[0;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[0;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[0;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[0;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[0;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[0;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[0;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[0;32m    556\u001b[0m     )\n\u001b[0;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[0;32m    672\u001b[0m     fn,\n\u001b[0;32m    673\u001b[0m     space,\n\u001b[0;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[0;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[0;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[0;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[0;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[0;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[0;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[0;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[0;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[0;32m    689\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[0;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[0;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[0;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[0;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[1;32md:\\Projetos\\PesquisaArtigo\\TCC_Hyperparameters Optimization\\TPE.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m epoc \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49mepoc, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Extract the loss from the training history\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/TPE.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m history_dict \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\keras\\src\\engine\\training.py:1774\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1772\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1773\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1774\u001b[0m     \u001b[39mfor\u001b[39;49;00m step \u001b[39min\u001b[39;49;00m data_handler\u001b[39m.\u001b[39;49msteps():\n\u001b[0;32m   1775\u001b[0m         \u001b[39mwith\u001b[39;49;00m tf\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mexperimental\u001b[39m.\u001b[39;49mTrace(\n\u001b[0;32m   1776\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1777\u001b[0m             epoch_num\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m             _r\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1781\u001b[0m         ):\n\u001b[0;32m   1782\u001b[0m             callbacks\u001b[39m.\u001b[39;49mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1411\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1412\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1413\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1414\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1416\u001b[0m )\n\u001b[0;32m   1418\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:687\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    686\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    688\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    689\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:814\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \n\u001b[0;32m    807\u001b[0m \u001b[39mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[39m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mRead\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 814\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n\u001b[0;32m    815\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:793\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    791\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    792\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    795\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    796\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    797\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    798\u001b[0m   record\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    799\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    800\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    801\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:783\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    782\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 783\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    785\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    786\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\anaconda3\\envs\\tcc\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:589\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    588\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    590\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    591\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    592\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for datasetNumber in range(startingDataset,21):\n",
    "\n",
    "    folderName = 'trainings/Fully_Connected_'+ samplingMethod\n",
    "    trainingFile = 'trainings/Fully_Connected_'+ samplingMethod +'/training_'+ str(startingDataset) +'.csv'\n",
    "    \n",
    "    print('Training: training_'+ str(startingDataset) +'.csv')\n",
    "    \n",
    "    # Create a CSV file if it doesn't exist to store the results\n",
    "    if not os.path.exists(trainingFile):\n",
    "        os.makedirs(folderName)\n",
    "        with open(trainingFile, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Hidden_Layer1\", \"Hidden_Layer2\", \"Learning_Rate\", \"Batch_Size\", \"Loss\"])\n",
    "            file.close()\n",
    "    \n",
    "    with open(trainingFile, 'r', newline='') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        line = sum(1 for row in csv_reader)\n",
    "        file.close()\n",
    "    \n",
    "    for training in range(line-1, trains, 1):\n",
    "    \n",
    "        # Create a Trials object to keep track of the optimization process\n",
    "        trials = Trials()\n",
    "    \n",
    "        # Use TPE to search for the best hyperparameters\n",
    "        best = fmin(fn=objective_function_MLP,\n",
    "                    space=spaceMLP,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=5,  # Adjust the number of evaluations as needed\n",
    "                    trials=trials)\n",
    "    \n",
    "        print(\"Best hyperparameters:\", best)\n",
    "    \n",
    "        best = pd.DataFrame.from_dict(best, orient='index')\n",
    "        best = best.transpose().reset_index()\n",
    "        best = best.drop('index', axis=1)\n",
    "    \n",
    "        losses = [trial['result']['loss'] for trial in trials.trials]\n",
    "        final_loss = np.min(losses)\n",
    "    \n",
    "        print(\"Name: {} Hidden_Layer1: {} Hidden_Layer2: {} Learning_Rate: {} Batch_Size: {} Loss: {}\".format(optimizerSelector.numberToName(int(best['Name'][0])),int(best['Hidden_Layer1'][0]),int(best['Hidden_Layer2'][0]),best['Learning_Rate'][0],int(best['Batch_Size'][0]),final_loss))\n",
    "    \n",
    "        with open(trainingFile, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([optimizerSelector.numberToName(int(best['Name'][0])),int(best['Hidden_Layer1'][0]),int(best['Hidden_Layer2'][0]),best['Learning_Rate'][0],int(best['Batch_Size'][0]),final_loss])\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datasetNumber in range(startingDataset,21):\n",
    "\n",
    "    folderName = 'trainings/CNN_'+ samplingMethod\n",
    "    trainingFile = 'trainings/CNN_'+ samplingMethod +'/training_'+ str(datasetNumber) +'.csv'\n",
    "\n",
    "    print('Training: training_'+ str(datasetNumber) +'.csv')\n",
    "\n",
    "    # Create a CSV file if it doesn't exist to store the results\n",
    "    if not os.path.exists(trainingFile):\n",
    "        os.makedirs(folderName)\n",
    "        with open(trainingFile, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Convoluted_Layers1\", \"Convoluted_Filters1\", \"Convoluted_Layers2\", \"Convoluted_Filters2\", \"Hidden_Layer1\", \"Hidden_Layer2\", \"Learning_Rate\", \"Batch_Size\", \"Loss\"])\n",
    "            file.close()\n",
    "\n",
    "    with open(trainingFile, 'r', newline='') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        line = sum(1 for row in csv_reader)\n",
    "        file.close()\n",
    "\n",
    "    for training in range(line-1, trains, 1):\n",
    "\n",
    "        # Create a Trials object to keep track of the optimization process\n",
    "        trials = Trials()\n",
    "\n",
    "        # Use TPE to search for the best hyperparameters\n",
    "        best = fmin(fn=objective_function_MLP,\n",
    "                    space=spaceMLP,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=5,  # Adjust the number of evaluations as needed\n",
    "                    trials=trials)\n",
    "\n",
    "        print(\"Best hyperparameters:\", best)\n",
    "\n",
    "        best = pd.DataFrame.from_dict(best, orient='index')\n",
    "        best = best.transpose().reset_index()\n",
    "        best = best.drop('index', axis=1)\n",
    "\n",
    "        losses = [trial['result']['loss'] for trial in trials.trials]\n",
    "        final_loss = np.min(losses)\n",
    "\n",
    "        with open(trainingFile, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([best['Name'], best['Convoluted_Layers1'],best['Convoluted_Filters1'],best['Convoluted_Layers2'],best['Convoluted_Filters2'],best['Hidden_Layer1'],best['Hidden_Layer2'],best['Learning_Rate'],best['Batch_Size'],final_loss])\n",
    "            file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
