{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 02:40:54.391140: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-20 02:40:54.391286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-20 02:40:54.403967: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-20 02:40:54.450634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 02:40:55.655672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50000, 32, 32, 3)\n",
      "Training labels shape: (50000, 1)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.datasets.cifar100 as cifar100\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Print the shape of the training data\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "\n",
    "# Print the shape of the training labels\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "\n",
    "# Print the shape of the test data\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "\n",
    "# Print the shape of the test labels\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 02:40:57.952544: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.193588: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.193660: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.197252: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.197341: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.197396: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.523657: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.523870: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.523887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-20 02:40:58.524026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 02:40:58.524132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4699 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:09:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 2, 2, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 1, 1, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               409700    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34006948 (129.73 MB)\n",
      "Trainable params: 34006948 (129.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the VGG16 model\n",
    "model = Sequential()\n",
    "\n",
    "# Block 1\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# Block 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# Block 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# Block 4\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# Block 5\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# Flatten the output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 02:41:01.775831: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-06-20 02:41:03.472616: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f6564241690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-20 02:41:03.472680: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1\n",
      "2024-06-20 02:41:03.501437: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718862063.712215  325055 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:905\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    910\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[1;32m    911\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/hyperparamsOPT/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from optimizerSelector import randomizeOptimizer\n",
    "import Sampling\n",
    "\n",
    "# Define the number of CNN models to train\n",
    "trains = 50000\n",
    "#trains2 = 50000\n",
    "dataSamples = 20\n",
    "for samples in range(0,dataSamples):\n",
    "\n",
    "    csvName = 'data/sobol/CNN_Sobol_Hyperparameters_'+ str(samples+1) +'.csv'\n",
    "\n",
    "    # Create a CSV file if it doesn't exist to store the results\n",
    "    if not os.path.exists(csvName):\n",
    "        with open(csvName, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Name\", \"Convoluted_Layers1\", \"Convoluted_Filters1\", \"Convoluted_Layers2\", \"Convoluted_Filters2\", \"Hidden_Layer1\", \"Hidden_Layer2\", \"Learning_Rate\", \"Batch_Size\"])\n",
    "            file.close()\n",
    "\n",
    "    # Generate random hyperparameters for CNN models\n",
    "    conv_n1 = Sampling.grid_sampling(trains, 16., 32.)\n",
    "    conv_f1 = Sampling.grid_sampling(trains, 1., 3.)\n",
    "    conv_n2 = Sampling.grid_sampling(trains, 16., 32.)\n",
    "    conv_f2 = Sampling.grid_sampling(trains, 1., 3.)\n",
    "    L1 = Sampling.grid_sampling(trains, 32., 64.)\n",
    "    L2 = Sampling.grid_sampling(trains, 16., 32.)\n",
    "    optimizer_number = Sampling.grid_sampling(trains, 1., 9.)\n",
    "    l_rate = Sampling.grid_sampling(trains, 0.0001, 0.01, round=False)\n",
    "    bt_size = Sampling.grid_sampling(trains, 32., 128.)\n",
    "\n",
    "    #print(len(optimizer_number))\n",
    "    #break\n",
    "    for training in range(0, trains2, 1):\n",
    "        print(\"Training:\", training + 1)\n",
    "        optimizer, name = randomizeOptimizer(optimizer_number[training], l_rate[training])\n",
    "    \n",
    "        with open(csvName, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([name, conv_n1[training], conv_f1[training], conv_n2[training], conv_f2[training], L1[training], L2[training], l_rate[training], bt_size[training]])\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Sampling\n",
    "\n",
    "\n",
    "# Define the range for each hyperparameter\n",
    "conv_n1_range = (16., 32.)\n",
    "conv_f1_range = (1., 3.)\n",
    "conv_n2_range = (16., 32.)\n",
    "conv_f2_range = (1., 3.)\n",
    "maxPooling1_range = (32., 64.)\n",
    "\n",
    "conv_n3_range = (16., 32.)\n",
    "conv_f3_range = (1., 3.)\n",
    "conv_n4_range = (16., 32.)\n",
    "conv_f4_range = (1., 3.)\n",
    "maxPooling2_range = (32., 64.)\n",
    "\n",
    "conv_n5_range = (16., 32.)\n",
    "conv_f5_range = (1., 3.)\n",
    "conv_n6_range = (16., 32.)\n",
    "conv_f6_range = (1., 3.)\n",
    "conv_n7_range = (16., 32.)\n",
    "conv_f7_range = (1., 3.)\n",
    "maxPooling3_range = (32., 64.)\n",
    "\n",
    "conv_n8_range = (16., 32.)\n",
    "conv_f8_range = (1., 3.)\n",
    "conv_n9_range = (16., 32.)\n",
    "conv_f9_range = (1., 3.)\n",
    "conv_n10_range = (16., 32.)\n",
    "conv_f10_range = (1., 3.)\n",
    "maxPooling4_range = (32., 64.)\n",
    "\n",
    "conv_n11_range = (16., 32.)\n",
    "conv_f11_range = (1., 3.)\n",
    "conv_n12_range = (16., 32.)\n",
    "conv_f12_range = (1., 3.)\n",
    "conv_n13_range = (16., 32.)\n",
    "conv_f13_range = (1., 3.)\n",
    "maxPooling5_range = (32., 64.)\n",
    "\n",
    "Dense1_range = (32., 64.)\n",
    "Dense2_range = (32., 64.)\n",
    "Dense3_range = (32., 64.)\n",
    "\n",
    "\n",
    "conv_n1 = Sampling.lhs_sampling(trains, 16., 32.)\n",
    "conv_f1 = Sampling.grid_sampling(trains, 1., 3.)\n",
    "conv_n2 = Sampling.grid_sampling(trains, 16., 32.)\n",
    "conv_f2 = Sampling.grid_sampling(trains, 1., 3.)\n",
    "optimizer_number = Sampling.grid_sampling(trains, 1., 9.)\n",
    "l_rate = Sampling.grid_sampling(trains, 0.0001, 0.01, round=False)\n",
    "bt_size = Sampling.grid_sampling(trains, 32., 128.)\n",
    "Dense3 = Sampling.grid_sampling(trains, Dense3_range, *Dense3_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16.0, 32.0) (16.0, 32.0) (32.0, 128.0) (32.0, 128.0)\n"
     ]
    }
   ],
   "source": [
    "conv_n1_range = [(16., 32.)]\n",
    "conv_n2_range = [(32., 128.)]\n",
    "conv_n1_range = (conv_n1_range*2)+conv_n2_range*2\n",
    "\n",
    "print(*conv_n1_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Sampling\n",
    "\n",
    "\n",
    "# Define the range for each hyperparameter\n",
    "# Block 1\n",
    "conv_n1_range = (32., 128.)\n",
    "conv_f1_range = (1., 3.)\n",
    "conv_n2_range = (32., 128.)\n",
    "conv_f2_range = (1., 3.)\n",
    "maxPooling1_range = (1., 3.)\n",
    "\n",
    "# Block 2\n",
    "conv_n3_range = (64., 256.)\n",
    "conv_f3_range = (1., 3.)\n",
    "conv_n4_range = (64., 256.)\n",
    "conv_f4_range = (1., 3.)\n",
    "maxPooling2_range = (1., 3.)\n",
    "\n",
    "# Block 3\n",
    "conv_n5_range = (128., 512.)\n",
    "conv_f5_range = (1., 3.)\n",
    "conv_n6_range = (128., 512.)\n",
    "conv_f6_range = (1., 3.)\n",
    "conv_n7_range = (128., 512.)\n",
    "conv_f7_range = (1., 3.)\n",
    "maxPooling3_range = (1., 3.)\n",
    "\n",
    "# Block 4\n",
    "conv_n8_range = (256., 1024.)\n",
    "conv_f8_range = (1., 3.)\n",
    "conv_n9_range = (256., 1024.)\n",
    "conv_f9_range = (1., 3.)\n",
    "conv_n10_range = (256., 1024.)\n",
    "conv_f10_range = (1., 3.)\n",
    "maxPooling4_range = (1., 3.)\n",
    "\n",
    "# Block 5\n",
    "conv_n11_range = (512., 2048.)\n",
    "conv_f11_range = (1., 3.)\n",
    "conv_n12_range = (512., 2048.)\n",
    "conv_f12_range = (1., 3.)\n",
    "conv_n13_range = (512., 2048.)\n",
    "conv_f13_range = (1., 3.)\n",
    "maxPooling5_range = (1., 3.)\n",
    "\n",
    "# Fully connected layers\n",
    "Dense1_range = (2048., 8192.)\n",
    "Dense2_range = (2048., 8192.)\n",
    "Dense3_range = (2048., 8192.)\n",
    "\n",
    "# Compilation\n",
    "optimizer_number_range = (1., 9.)\n",
    "l_rate_range = (0.0001, 0.01)\n",
    "bt_size_range = (32., 128.)\n",
    "\n",
    "\n",
    "\n",
    "# Define the number of samples to generate\n",
    "trains = 100000\n",
    "\n",
    "conv_n1 = Sampling.grid_sampling(trains, *conv_n1_range)\n",
    "conv_f1 = Sampling.grid_sampling(trains, *conv_f1_range)\n",
    "conv_n2 = Sampling.grid_sampling(trains, *conv_n2_range)\n",
    "conv_f2 = Sampling.grid_sampling(trains, *conv_f2_range)\n",
    "maxPooling1 = Sampling.grid_sampling(trains, *maxPooling1_range)\n",
    "\n",
    "conv_n3 = Sampling.grid_sampling(trains, *conv_n3_range)\n",
    "conv_f3 = Sampling.grid_sampling(trains, *conv_f3_range)\n",
    "conv_n4 = Sampling.grid_sampling(trains, *conv_n4_range)\n",
    "conv_f4 = Sampling.grid_sampling(trains, *conv_f4_range)\n",
    "maxPooling2 = Sampling.grid_sampling(trains, *maxPooling2_range)\n",
    "\n",
    "conv_n5 = Sampling.grid_sampling(trains, *conv_n5_range)\n",
    "conv_f5 = Sampling.grid_sampling(trains, *conv_f5_range)\n",
    "conv_n6 = Sampling.grid_sampling(trains, *conv_n6_range)\n",
    "conv_f6 = Sampling.grid_sampling(trains, *conv_f6_range)\n",
    "conv_n7 = Sampling.grid_sampling(trains, *conv_n7_range)\n",
    "conv_f7 = Sampling.grid_sampling(trains, *conv_f7_range)\n",
    "maxPooling3 = Sampling.grid_sampling(trains, *maxPooling3_range)\n",
    "\n",
    "conv_n8 = Sampling.grid_sampling(trains, *conv_n8_range)\n",
    "conv_f8 = Sampling.grid_sampling(trains, *conv_f8_range)\n",
    "conv_n9 = Sampling.grid_sampling(trains, *conv_n9_range)\n",
    "conv_f9 = Sampling.grid_sampling(trains, *conv_f9_range)\n",
    "conv_n10 = Sampling.grid_sampling(trains,  *conv_n10_range)\n",
    "conv_f10 = Sampling.grid_sampling(trains,  *conv_f10_range)\n",
    "maxPooling4 = Sampling.grid_sampling(trains, *maxPooling4_range)\n",
    "\n",
    "conv_n11 = Sampling.grid_sampling(trains, *conv_n11_range)\n",
    "conv_f11 = Sampling.grid_sampling(trains, *conv_f11_range)\n",
    "conv_n12 = Sampling.grid_sampling(trains, *conv_n12_range)\n",
    "conv_f12 = Sampling.grid_sampling(trains, *conv_f12_range)\n",
    "conv_n13 = Sampling.grid_sampling(trains, *conv_n13_range)\n",
    "conv_f13 = Sampling.grid_sampling(trains, *conv_f13_range)\n",
    "maxPooling5 = Sampling.grid_sampling(trains, *maxPooling5_range)\n",
    "\n",
    "Dense1 = Sampling.grid_sampling(trains, *Dense1_range)\n",
    "Dense2 = Sampling.grid_sampling(trains, *Dense2_range)\n",
    "Dense3 = Sampling.grid_sampling(trains, *Dense3_range)\n",
    "\n",
    "optimizer_number = Sampling.grid_sampling(trains, *optimizer_number_range)\n",
    "l_rate = Sampling.grid_sampling(trains, *l_rate_range)\n",
    "bt_size = Sampling.grid_sampling(trains, *bt_size_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer_number: 50159\n"
     ]
    }
   ],
   "source": [
    "print(\"optimizer_number:\", len(optimizer_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizerSelector import randomizeOptimizer\n",
    "import csv\n",
    "\n",
    "for training in range(0, trains):\n",
    "    print(\"Training:\", training)\n",
    "    optimizer, name = randomizeOptimizer(optimizer_number[training], l_rate[training])\n",
    "    print(name)\n",
    "    with open(csvName, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([name, conv_n1[training], conv_f1[training], conv_n2[training], conv_f2[training], L1[training], L2[training], l_rate[training], bt_size[training]])\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generateHyperparameters.__init__() missing 4 required positional arguments: 'samplingMethod', 'optimizer', 'learningRate', and 'batchSize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,dataSamples):\n\u001b[1;32m     17\u001b[0m     csvName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39msampleMethod\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mmodel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39msampleMethod\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Hyperparameters_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(samples\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerateHyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainQuantity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleMethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     results \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgenerateVGG16(denseLayers\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2048.\u001b[39m, \u001b[38;5;241m8192.\u001b[39m),maxPoolingLayers\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m3.\u001b[39m),convolutedLayers\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;241m32.\u001b[39m, \u001b[38;5;241m128.\u001b[39m),(\u001b[38;5;241m64.\u001b[39m, \u001b[38;5;241m256.\u001b[39m),(\u001b[38;5;241m128.\u001b[39m, \u001b[38;5;241m512.\u001b[39m),(\u001b[38;5;241m256.\u001b[39m, \u001b[38;5;241m1024.\u001b[39m),(\u001b[38;5;241m512.\u001b[39m, \u001b[38;5;241m2048.\u001b[39m)],filterLayers\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m3.\u001b[39m))\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Generate random hyperparameters for CNN models\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: generateHyperparameters.__init__() missing 4 required positional arguments: 'samplingMethod', 'optimizer', 'learningRate', and 'batchSize'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from optimizerSelector import randomizeOptimizer\n",
    "import generateHyperparameters as gen\n",
    "\n",
    "\n",
    "# Define the number of CNN models to train\n",
    "trainQuantity = 10\n",
    "sampleMethod = 'grid'\n",
    "dataSamples = 1\n",
    "model = 'VGG16'\n",
    "for samples in range(0,dataSamples):\n",
    "\n",
    "    csvName = 'data/'+sampleMethod+'/'+model+'_'+sampleMethod+'_Hyperparameters_'+ str(samples+1) +'.csv'\n",
    "\n",
    "    generator = gen.generateHyperparameters(trainQuantity, sampleMethod)\n",
    "\n",
    "    results = generator.generateVGG16(denseLayers=(2048., 8192.),maxPoolingLayers=(1., 3.),convolutedLayers=[(32., 128.),(64., 256.),(128., 512.),(256., 1024.),(512., 2048.)],filterLayers=[(1., 3.)*])\n",
    "    # Generate random hyperparameters for CNN models\n",
    "   \n",
    "\n",
    "    print(results)\n",
    "\n",
    "    for training in range(0, trains2, 1):\n",
    "        print(\"Training:\", training + 1)\n",
    "        optimizer, name = randomizeOptimizer(optimizer_number[training], l_rate[training])\n",
    "    \n",
    "        with open(csvName, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([name, conv_n1[training], conv_f1[training], conv_n2[training], conv_f2[training], L1[training], L2[training], l_rate[training], bt_size[training]])\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definição da função de Rastrigin\n",
    "def rastrigin(X):\n",
    "    # Calcula o valor da função de Rastrigin para um vetor X\n",
    "    return 10 * len(X) + sum([x**2 - 10 * np.cos(2 * np.pi * x) for x in X])\n",
    "\n",
    "# Classe PSO\n",
    "class PSO:\n",
    "    def __init__(self, n_particulas, dimensoes, n_iteracoes, limites, w_min=0.1, w_max=0.9, v_max=0.1):\n",
    "        self.n_particulas = n_particulas  # Define o número de partículas\n",
    "        self.dimensoes = dimensoes  # Define a quantidade de dimensões do espaço de busca\n",
    "        self.n_iteracoes = n_iteracoes  # Define o número total de iterações\n",
    "        self.limites = limites  # Define os limites mínimos e máximos do espaço de busca\n",
    "        self.w_min = w_min  # Define o fator de inércia mínimo\n",
    "        self.w_max = w_max  # Define o fator de inércia máximo\n",
    "        self.v_max = v_max  # Define a velocidade máxima para a atualização da velocidade das partículas\n",
    "\n",
    "        # Inicializa as posições das partículas de forma aleatória dentro dos limites especificados\n",
    "        self.posicoes = np.random.rand(n_particulas, dimensoes) * (limites[1] - limites[0]) + limites[0]\n",
    "        # Inicializa as velocidades das partículas como zero\n",
    "        self.velocidades = np.zeros((n_particulas, dimensoes))\n",
    "        # Inicializa as melhores posições pessoais das partículas como suas posições iniciais\n",
    "        self.pbest_posicoes = self.posicoes.copy()\n",
    "        # Inicializa os melhores scores pessoais como infinito, pois ainda não foram avaliados\n",
    "        self.pbest_scores = np.array([float('inf')] * n_particulas)\n",
    "        # Inicializa o melhor score global como infinito\n",
    "        self.gbest_score = float('inf')\n",
    "        # Inicializa a melhor posição global como um vetor de infinitos\n",
    "        self.gbest_posicao = np.array([float('inf'), float('inf')])\n",
    "\n",
    "    # Método para atualizar a velocidade das partículas\n",
    "    def atualiza_velocidade(self, iteracao):\n",
    "        # Calcula o fator de inércia baseado na iteração atual\n",
    "        w = self.w_max - (self.w_max - self.w_min) * (iteracao / self.n_iteracoes)\n",
    "        for i in range(self.n_particulas):\n",
    "            r1, r2 = np.random.rand(2)\n",
    "            # Atualiza a velocidade da partícula baseado nas melhores posições pessoais e global\n",
    "            nova_velocidade = (w * self.velocidades[i] +\n",
    "                               r1 * (self.pbest_posicoes[i] - self.posicoes[i]) +\n",
    "                               r2 * (self.gbest_posicao - self.posicoes[i]))\n",
    "            # Aplica o limite de velocidade\n",
    "            self.velocidades[i] = np.clip(nova_velocidade, -self.v_max, self.v_max)\n",
    "\n",
    "    # Método para atualizar a posição das partículas e aplicar o método Damping\n",
    "    def atualiza_posicao(self):\n",
    "        for i in range(self.n_particulas):\n",
    "            # Atualiza a posição da partícula baseado em sua velocidade\n",
    "            self.posicoes[i] += self.velocidades[i]\n",
    "            # Para cada dimensão, verifica se a partícula excedeu os limites\n",
    "            for dim in range(self.dimensoes):\n",
    "                if self.posicoes[i][dim] < self.limites[0]:\n",
    "                    # Se menor que o limite mínimo, ajusta para o limite e inverte a velocidade\n",
    "                    self.posicoes[i][dim] = self.limites[0]\n",
    "                    self.velocidades[i][dim] *= -1\n",
    "                elif self.posicoes[i][dim] > self.limites[1]:\n",
    "                    # Se maior que o limite máximo, ajusta para o limite e inverte a velocidade\n",
    "                    self.posicoes[i][dim] = self.limites[1]\n",
    "                    self.velocidades[i][dim] *= -1\n",
    "\n",
    "    # Método para avaliar a função objetivo nas posições das partículas\n",
    "    def avalia(self):\n",
    "        for i in range(self.n_particulas):\n",
    "            # Calcula o score (valor da função objetivo) da partícula\n",
    "            score = rastrigin(self.posicoes[i])\n",
    "            # Se o score atual é melhor que o melhor pessoal, atualiza o melhor pessoal\n",
    "            if score < self.pbest_scores[i]:\n",
    "                self.pbest_scores[i] = score\n",
    "                self.pbest_posicoes[i] = self.posicoes[i].copy()\n",
    "            # Se o score atual é melhor que o melhor global, atualiza o melhor global\n",
    "            if score < self.gbest_score:\n",
    "                self.gbest_score = score\n",
    "                self.gbest_posicao = self.posicoes[i].copy()\n",
    "\n",
    "    # Método para executar o algoritmo PSO\n",
    "    def executa(self):\n",
    "        for iteracao in range(self.n_iteracoes):\n",
    "            self.atualiza_velocidade(iteracao)\n",
    "            self.atualiza_posicao()\n",
    "            self.avalia()\n",
    "            print(f\"Iteração {iteracao+1}/{self.n_iteracoes}, Melhor Score Global: {self.gbest_score}\")\n",
    "        return self.gbest_posicao, self.gbest_score\n",
    "\n",
    "# Parâmetros para a execução do PSO\n",
    "n_particulas = 30\n",
    "dimensoes = 2\n",
    "n_iteracoes = 100\n",
    "limites = (-5.12, 5.12)\n",
    "v_max = 0.2  # Limite máximo para a velocidade das partículas\n",
    "\n",
    "# Executa o PSO\n",
    "pso = PSO(n_particulas, dimensoes, n_iteracoes, limites, v_max=v_max)\n",
    "melhor_posicao, melhor_score = pso.executa()\n",
    "print(f\"Melhor posição: {melhor_posicao}, Melhor score: {melhor_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generateHyperparameters as gen\n",
    "\n",
    "\n",
    "# Define the number of CNN models to train\n",
    "trainQT = 10\n",
    "method= 'grid'\n",
    "dataSamples = 1\n",
    "model = 'VGG16'\n",
    "for samples in range(0,dataSamples):\n",
    "\n",
    "    csvName = 'data/'+method+'/'+model+'_'+method+'_Hyperparameters_'+ str(samples+1) +'.csv'\n",
    "\n",
    "    generator = gen.generateHyperparameters(trainQuantity=trainQT, samplingMethod=method)\n",
    "    convLayers=(32., 128.),(32., 128.),(64., 256.),(64., 256.),(128., 512.),(128., 512.),(128., 512.),(256., 1024.),(256., 1024.),(256., 1024.),(512., 2048.),(512., 2048.),(512., 2048.)\n",
    "    print(convLayers)\n",
    "    denLayers=[(2048., 8192.)]*3\n",
    "    print(denLayers)\n",
    "    filLayers=[(1., 3.)]*len(convLayers)\n",
    "    print(filLayers)\n",
    "    maxPLayers=[(1., 3.)]*len(convLayers)\n",
    "\n",
    "    results = generator.generateVGG16(convolutedLayers=convLayers,denseLayers=[(2048., 8192.)]*3,filterLayers=[(1., 3.)]*len(convLayers),maxPoolingLayers=[(1., 3.)]*len(convLayers))\n",
    "    # Generate random hyperparameters for CNN models\n",
    "   \n",
    "\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainQT = 5\n",
    "method= 'grid'\n",
    "dataSamples = 1\n",
    "model = 'VGG16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 10:33:59.339657: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-20 10:33:59.339832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-20 10:33:59.341167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-20 10:33:59.350801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 10:34:00.405667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import generateHyperparameters as gen\n",
    "from optimizerSelector import numberToName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "learning rate size:  0\n",
      "Ftrl\n",
      "learning rate:  0\n",
      "batch size:  128\n",
      "convoluted layers:  [1024, 256, 1024, 640, 256]\n",
      "dense layers:  [3, 2, 3, 1, 1]\n",
      "filter layers:  [1, 3, 3, 1, 2]\n",
      "max pooling layers:  [8192, 5120, 2048, 2048, 8192]\n",
      "Adadelta\n",
      "learning rate:  0\n",
      "batch size:  128\n",
      "convoluted layers:  [128, 512, 512, 128, 320]\n",
      "dense layers:  [3, 1, 3, 2, 1]\n",
      "filter layers:  [1, 2, 3, 3, 1]\n",
      "max pooling layers:  [8192, 5120, 2048, 2048, 8192]\n",
      "SGD\n",
      "learning rate:  0\n",
      "batch size:  80\n",
      "convoluted layers:  [80, 128, 128, 32, 32]\n",
      "dense layers:  [3, 1, 2, 1, 3]\n",
      "filter layers:  [3, 3, 1, 1, 2]\n",
      "max pooling layers:  [8192, 2048, 5120, 2048, 8192]\n",
      "Ftrl\n",
      "learning rate:  0\n",
      "batch size:  128\n",
      "convoluted layers:  [1024, 256, 1024, 640, 256]\n",
      "dense layers:  [3, 2, 3, 1, 1]\n",
      "filter layers:  [1, 3, 3, 1, 2]\n",
      "max pooling layers:  [8192, 5120, 2048, 2048, 8192]\n",
      "SGD\n",
      "learning rate:  0\n",
      "batch size:  80\n",
      "convoluted layers:  [80, 128, 128, 32, 32]\n",
      "dense layers:  [3, 1, 2, 1, 3]\n",
      "filter layers:  [3, 3, 1, 1, 2]\n",
      "max pooling layers:  [8192, 2048, 5120, 2048, 8192]\n"
     ]
    }
   ],
   "source": [
    "generator = gen.generateHyperparameters(trainQuantity=trainQT, samplingMethod=method,optimizer=(0., 8.))\n",
    "convLayers=[(32., 128.),(32., 128.),(64., 256.),(64., 256.),(128., 512.),(128., 512.),(128., 512.),(256., 1024.),(256., 1024.),(256., 1024.),(512., 2048.),(512., 2048.),(512., 2048.)]\n",
    "denLayers=[(2048., 8192.)]*3\n",
    "filLayers=[(1., 3.)]*len(convLayers)\n",
    "maxPLayers=[(1., 3.)]*len(convLayers)\n",
    "\n",
    "optimizer,learningRate,batchSize,convolutedLayers,denseLayers,filterLayers,maxPoolingLayers = generator.generateVGG16(convolutedLayers=convLayers,denseLayers=[(2048., 8192.)]*3,filterLayers=[(1., 3.)]*len(convLayers),maxPoolingLayers=[(1., 3.)]*len(convLayers))\n",
    "    # Generate random hyperparameters for CNN models\n",
    "print(learningRate)\n",
    "print(\"learning rate size: \", *[learningRate[0].tolist()])\n",
    "\n",
    "\n",
    "for training in optimizer:\n",
    "        name = numberToName(training)\n",
    "        print(name)\n",
    "\n",
    "        print(\"learning rate: \", *[learningRate[min(training, len(learningRate)-1)].tolist()])\n",
    "        print(\"batch size: \", *[batchSize[min(training, len(batchSize)-1)].tolist()])\n",
    "        print(\"convoluted layers: \", *[convolutedLayers[min(training, len(convolutedLayers)-1)].tolist()])\n",
    "        print(\"dense layers: \", *[denseLayers[min(training, len(denseLayers)-1)].tolist()])\n",
    "        print(\"filter layers: \", *[filterLayers[min(training, len(filterLayers)-1)].tolist()])\n",
    "        print(\"max pooling layers: \", *[maxPoolingLayers[min(training, len(maxPoolingLayers)-1)].tolist()])\n",
    "        #print(name, learningRate[samples][training], batchSize[samples][training], convolutedLayers[samples][training], denseLayers[samples][training], filterLayers[samples][training], maxPoolingLayers[samples][training])\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
