Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
AdamW,56,26,0.01,128,0.07619402557611465
AdamW,64,32,0.01,105,0.07641042768955231
RMSprop,64,26,0.01,98,0.08837834000587463
AdamW,21,25,0.01,85,0.12797755002975464
Ftrl,64,28,0.01,116,2.301192283630371
Adagrad,62,32,0.01,95,0.2953933775424957
Adamax,64,23,0.01,109,0.07605279982089996
Nadam,61,32,0.01,128,0.07001753896474838
Adadelta,62,31,0.01,120,1.5121411085128784
Ftrl,64,27,0.01,96,2.301192283630371
Adadelta,46,32,0.01,91,1.3806151151657104
Ftrl,55,30,0.01,94,2.301192283630371
Adagrad,61,8,0.01,88,0.4310356378555298
Adagrad,62,32,0.01,128,0.3217073976993561
Ftrl,64,32,0.01,128,2.3011937141418457
Adadelta,64,23,0.01,67,1.1119418144226074
Ftrl,60,18,0.01,100,2.3011929988861084
Nadam,38,29,0.01,97,0.09627488255500793
Ftrl,52,23,0.01,101,2.3011910915374756
AdamW,64,26,0.01,117,0.07623407244682312
