Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Best_Value
Adagrad,55,19,0.01,54,-0.34124618059656453
Adamax,64,22,0.01,122,-0.3323490735259951
Adam,55,13,0.01,119,-0.33813252528187426
Ftrl,55,29,0.01,103,-0.3485297857405869
Adamax,56,28,0.01,128,-0.3381552071257267
RMSprop,54,29,0.01,117,-0.3418062162771569
Ftrl,43,24,0.01,128,-0.3389799139685377
Ftrl,64,23,0.01,97,-0.3385376423009934
RMSprop,35,32,0.01,63,-0.3366830590802446
Ftrl,42,32,0.01,57,-0.33921849515237734
Adagrad,55,26,0.01,51,-0.3479073386155066
Nadam,58,32,0.01,77,-0.3328307918754939
Nadam,58,28,0.01,85,-0.33978213303720384
Ftrl,64,26,0.01,128,-0.3490306081947684
Ftrl,56,32,0.01,111,-0.3341447127717213
Ftrl,59,32,0.01,128,-0.33103632871657307
Ftrl,49,22,0.01,128,-0.33068084781013984
Adadelta,47,32,0.01,88,-0.32829424539340724
Adam,49,18,0.01,54,-0.34910919570282584
Adadelta,36,32,0.01,95,-0.33432720882346995
