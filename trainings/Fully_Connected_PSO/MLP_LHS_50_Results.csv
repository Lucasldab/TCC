Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
Ftrl,37,23,0.01,52,2.3011887073516846
Nadam,45,32,0.01,62,0.08471981436014175
Nadam,64,22,0.01,123,0.07367727905511856
Ftrl,59,30,0.01,100,2.301192045211792
Ftrl,41,32,0.01,97,2.30118989944458
Adamax,49,21,0.01,120,0.08953111618757248
Nadam,64,28,0.01,90,0.07265235483646393
Nadam,64,32,0.01,128,0.0658317282795906
Ftrl,64,24,0.01,77,2.3011889457702637
Adamax,61,32,0.01,124,0.07006877660751343
Adam,48,32,0.01,128,0.08264656364917755
Nadam,64,24,0.01,58,0.08209308236837387
Adadelta,44,21,0.01,118,1.5498487949371338
Nadam,64,27,0.01,79,0.07852310687303543
Adagrad,49,32,0.01,125,0.3188247084617615
Ftrl,45,28,0.01,115,2.301192045211792
Ftrl,62,32,0.01,64,2.3011910915374756
Ftrl,59,28,0.01,82,2.3011889457702637
Adadelta,39,26,0.01,65,1.1858187913894653
Adadelta,52,32,0.01,107,1.195918083190918
