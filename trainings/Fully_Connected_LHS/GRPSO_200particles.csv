Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Best_Value
Ftrl,64,20,0.01,89,-0.350026111718789
Adadelta,45,27,0.01,67,-0.3389991384750377
Adamax,60,27,0.01,128,-0.3390468023497748
Ftrl,64,32,0.01,51,-0.32955633260694556
Ftrl,43,24,0.01,128,-0.3492200451915862
Adam,52,30,0.01,128,-0.3362918994736942
Ftrl,58,26,0.01,100,-0.33596581888086496
Ftrl,44,32,0.01,98,-0.34842616857060754
Adagrad,57,25,0.01,115,-0.351302504359397
Nadam,55,29,0.01,128,-0.3363809548487696
Nadam,64,32,0.01,128,-0.3333596858125374
Adam,54,24,0.01,59,-0.3490144698365265
Adamax,64,23,0.01,122,-0.33986961116352166
Adagrad,64,30,0.01,74,-0.3287647318833623
Ftrl,44,32,0.01,63,-0.3508309544376242
AdamW,57,30,0.01,87,-0.3371838759980696
Adagrad,64,23,0.01,41,-0.337671026290146
Ftrl,49,23,0.01,69,-0.33328198043986906
Ftrl,63,29,0.01,128,-0.331380969259385
Ftrl,63,32,0.01,19,-0.3334824850360957
