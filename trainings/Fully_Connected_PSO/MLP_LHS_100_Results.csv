Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
Ftrl,64,32,0.01,97,2.30119252204895
Ftrl,50,24,0.01,85,2.3011908531188965
Adadelta,42,27,0.01,128,1.4194544553756714
Ftrl,64,23,0.01,106,2.30118989944458
Ftrl,59,32,0.01,109,2.3011951446533203
Adadelta,46,32,0.01,128,1.6070115566253662
AdamW,64,27,0.01,128,0.07466640323400497
Adadelta,42,31,0.01,125,1.5399460792541504
AdamW,30,31,0.01,97,0.10680399090051651
Ftrl,64,32,0.01,114,2.301192045211792
Ftrl,60,32,0.01,98,2.301189422607422
Ftrl,46,30,0.01,118,2.301192283630371
Adadelta,31,21,0.01,125,1.6873267889022827
Adam,47,28,0.01,88,0.08314917236566544
Adamax,62,21,0.01,91,0.07739546149969101
Ftrl,56,31,0.01,86,2.3011908531188965
Adam,60,22,0.01,79,0.08450616151094437
Adadelta,45,27,0.01,120,1.4986063241958618
Ftrl,49,25,0.01,99,2.3011927604675293
Ftrl,64,28,0.01,74,2.3011903762817383
