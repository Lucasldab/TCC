Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
AdamW,62,16,0.01,69,0.0896034836769104
AdamW,54,32,0.01,38,0.10655809193849564
Adam,52,32,0.01,52,0.09719360619783401
AdamW,62,28,0.01,75,0.08221352845430374
RMSprop,50,32,0.01,98,0.09519581496715546
AdamW,62,21,0.01,128,0.07147471606731415
Adadelta,49,32,0.01,89,1.1763222217559814
AdamW,47,23,0.01,89,0.08965381979942322
Adam,45,32,0.01,120,0.08384007215499878
RMSprop,58,32,0.01,43,0.1559310406446457
RMSprop,64,32,0.01,107,0.08527734875679016
Adam,64,22,0.01,88,0.07855329662561417
Adam,58,11,0.01,128,0.07936626672744751
Adam,62,30,0.01,68,0.08847168833017349
Adagrad,52,31,0.01,67,0.2812767028808594
Adam,55,27,0.01,51,0.0943690612912178
AdamW,59,32,0.01,89,0.07722415030002594
Adam,64,18,0.01,59,0.08715038001537323
AdamW,61,32,0.01,115,0.07827742397785187
Adadelta,63,32,0.01,106,1.0972989797592163
