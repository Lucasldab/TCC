Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
Ftrl,64,20,0.01,89,2.3011908531188965
Adadelta,45,27,0.01,67,1.2358202934265137
Adamax,60,27,0.01,128,0.07353119552135468
Ftrl,64,32,0.01,51,2.3011884689331055
Ftrl,43,24,0.01,128,2.3011929988861084
Adam,52,30,0.01,128,0.0787866860628128
Ftrl,58,26,0.01,100,2.301191568374634
Ftrl,44,32,0.01,98,2.301190137863159
Adagrad,57,25,0.01,115,0.3081630766391754
Nadam,55,29,0.01,128,0.07126428931951523
Nadam,64,32,0.01,128,0.06805938482284546
Adam,54,24,0.01,59,0.08733770251274109
Adamax,64,23,0.01,122,0.07077810913324356
Adagrad,64,30,0.01,74,0.2711801826953888
Ftrl,44,32,0.01,63,2.30118989944458
AdamW,57,30,0.01,87,0.08488673716783524
Adagrad,64,23,0.01,41,0.23529942333698273
Ftrl,49,23,0.01,69,2.30118989944458
Ftrl,63,29,0.01,128,2.301194190979004
Ftrl,63,32,0.01,19,2.3011908531188965
