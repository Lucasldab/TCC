{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data\\grid\\CNN_Grid_Hyperparameters_20.csv\")\n",
    "\n",
    "# Generate random hyperparameters for CNN models\n",
    "conv_n1 = data[\"Convoluted_Layers1\"].values\n",
    "conv_f1 = data[\"Convoluted_Filters1\"].values\n",
    "conv_n2 = data[\"Convoluted_Layers2\"].values\n",
    "conv_f2 = data[\"Convoluted_Filters2\"].values\n",
    "L1 = data[\"Hidden_Layer1\"].values\n",
    "L2 = data[\"Hidden_Layer2\"].values\n",
    "optimizer_name = data[\"Name\"].values\n",
    "l_rate = data[\"Learning_Rate\"].values\n",
    "bt_size = data[\"Batch_Size\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038328845729419"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rate[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defining_optimizer_byName(optimizer,l_rate):\n",
    "    if (optimizer == 'SGD'):\n",
    "        optimizer = optimizers.SGD(learning_rate=l_rate)\n",
    "        name = optimizers.SGD.__name__\n",
    "    elif (optimizer == 'RMSprop'):\n",
    "        optimizer = optimizers.RMSprop(learning_rate=l_rate)\n",
    "        name = optimizers.RMSprop.__name__\n",
    "    elif (optimizer == 'Adam'):\n",
    "        optimizer = optimizers.Adam(learning_rate=l_rate)\n",
    "        name = optimizers.Adam.__name__\n",
    "    elif (optimizer == 'AdamW'):\n",
    "        optimizer = optimizers.AdamW(learning_rate=l_rate)\n",
    "        name = optimizers.AdamW.__name__\n",
    "    elif (optimizer == 'Adadelta'):\n",
    "        optimizer = optimizers.Adadelta(learning_rate=l_rate)\n",
    "        name = optimizers.Adadelta.__name__\n",
    "    elif (optimizer == 'Adagrad'):\n",
    "        optimizer = optimizers.Adagrad(learning_rate=l_rate)\n",
    "        name = optimizers.Adagrad.__name__\n",
    "    elif (optimizer == 'Adamax'):\n",
    "        optimizer = optimizers.Adamax(learning_rate=l_rate)\n",
    "        name = optimizers.Adamax.__name__\n",
    "    elif (optimizer == 'Nadam'):\n",
    "        optimizer = optimizers.Nadam(learning_rate=l_rate)\n",
    "        name = optimizers.Nadam.__name__\n",
    "    elif (optimizer == 'Ftrl'):\n",
    "        optimizer = optimizers.Ftrl(learning_rate=l_rate)\n",
    "        name = optimizers.Ftrl.__name__\n",
    "    return optimizer, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(input(\"Which row it starts: \"))\n",
    "trains = int(input(\"CNN numbers: \"))\n",
    "datasetNumber = 5\n",
    "samplingMethod = 'LHS'\n",
    "datasetLocal = 'data/'+ samplingMethod +'/CNN_'+ samplingMethod +'_Hyperparameters_'+ str(datasetNumber) +'.csv'\n",
    "trainingFile = 'trainings/CNN_'+ samplingMethod +'/training_'+ str(datasetNumber) +'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(start)\n",
    "print(trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file if it doesn't exist to store the results\n",
    "if not os.path.exists(trainingFile):\n",
    "    with open(trainingFile, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Name\", \"Convoluted_Layers1\", \"Convoluted_Filters1\", \"Convoluted_Layers2\", \"Convoluted_Filters2\", \"Hidden_Layer1\", \"Hidden_Layer2\", \"Learning_Rate\", \"Batch_Size\", \"Loss\"])\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(datasetLocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random hyperparameters for CNN models\n",
    "conv_n1 = data[\"Convoluted_Layers1\"].values\n",
    "conv_f1 = data[\"Convoluted_Filters1\"].values\n",
    "conv_n2 = data[\"Convoluted_Layers2\"].values\n",
    "conv_f2 = data[\"Convoluted_Filters2\"].values\n",
    "L1 = data[\"Hidden_Layer1\"].values\n",
    "L2 = data[\"Hidden_Layer2\"].values\n",
    "optimizer_name = data[\"Name\"].values\n",
    "l_rate = data[\"Learning_Rate\"].values\n",
    "bt_size = data[\"Batch_Size\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'optimizer_selector' has no attribute 'defining_optimizer_byName'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Projetos\\PesquisaArtigo\\TCC_Hyperparameters Optimization\\teste.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/teste.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDense(\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/teste.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Compile the model with a random optimizer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/teste.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m optimizer, name \u001b[39m=\u001b[39m optimizer_selector\u001b[39m.\u001b[39;49mdefining_optimizer_byName(optimizer[training], l_rate[training])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/teste.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/PesquisaArtigo/TCC_Hyperparameters%20Optimization/teste.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m epoc \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'optimizer_selector' has no attribute 'defining_optimizer_byName'"
     ]
    }
   ],
   "source": [
    "# Loop over the specified number of training runs\n",
    "for training in range(start, trains, 1):\n",
    "    print(\"Training:\", training + 1)\n",
    "\n",
    "    # Load and preprocess MNIST data\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape((60000, 28, 28, 1))\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.reshape((10000, 28, 28, 1))\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    # Create a Sequential model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    model.add(layers.Conv2D(int(conv_n1[training]), (int(conv_f1[training])), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(int(conv_n2[training]), (int(conv_f2[training])), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Add fully connected layers\n",
    "    model.add(layers.Dense(L1[training], activation='relu'))\n",
    "    model.add(layers.Dense(L2[training], activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model with a random optimizer\n",
    "    optimizer, name = optimizer_selector.defining_optimizer_byName(optimizer_name[training], l_rate[training])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    epoc = 5\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=epoc, batch_size=bt_size[training])\n",
    "\n",
    "    # Extract the loss from the training history\n",
    "    history_dict = history.history\n",
    "    final_loss = history_dict['loss'][epoc-1]\n",
    "\n",
    "    # Print and save model and training information\n",
    "    print(\"Optimizer: {} Convoluted Layers 1: {} Convoluted Filters 1: {} Convoluted Layers 2: {} Convoluted Filters 2: {} Hidden Layer 1: {} Hidden Layer 2: {} Learning Rate: {} Batch Size: {} Loss: {}\".format(name, conv_n1[training], conv_f1[training], conv_n2[training], conv_f2[training], L1[training], L2[training], l_rate[training], bt_size[training], final_loss))\n",
    "    \n",
    "    with open(trainingFile, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([name, conv_n1[training], conv_f1[training], conv_n2[training], conv_f2[training], L1[training], L2[training], l_rate[training], bt_size[training], final_loss])\n",
    "        file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
