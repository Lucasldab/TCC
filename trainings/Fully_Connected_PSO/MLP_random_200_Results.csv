Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
Adam,51,28,0.01,99,0.08751717209815979
Adadelta,57,29,0.01,52,0.9369003176689148
Adamax,63,28,0.01,128,0.0689593181014061
Adam,39,32,0.01,128,0.08838740736246109
Ftrl,64,28,0.01,115,2.301192045211792
Adagrad,37,30,0.01,58,0.27685558795928955
Adagrad,64,31,0.01,128,0.3131856322288513
Nadam,64,25,0.01,128,0.06489989906549454
Ftrl,44,25,0.01,76,2.3011910915374756
Adamax,64,32,0.01,100,0.070198193192482
Ftrl,57,32,0.01,128,2.301193952560425
Adagrad,51,26,0.01,115,0.31465306878089905
Ftrl,64,17,0.01,128,2.3011937141418457
AdamW,17,32,0.01,59,0.16513755917549133
Ftrl,64,31,0.01,99,2.301192283630371
Ftrl,59,19,0.01,93,2.3011932373046875
Adadelta,64,26,0.01,51,0.9048136472702026
Adagrad,64,22,0.01,117,0.3351742625236511
Ftrl,60,31,0.01,34,2.301190137863159
Adadelta,55,32,0.01,60,0.9614447951316833
