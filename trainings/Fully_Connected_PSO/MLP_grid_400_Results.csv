Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Loss
Adagrad,60,18,0.01,122,0.3335888683795929
Adagrad,46,22,0.01,54,0.28262805938720703
Adagrad,42,30,0.01,42,0.25479844212532043
Adamax,46,26,0.01,117,0.08774064481258392
Adamax,46,22,0.01,66,0.07831243425607681
Adam,44,30,0.01,49,0.1035291850566864
Adadelta,61,28,0.01,57,0.9742347598075867
Adam,63,27,0.01,77,0.0777779296040535
Adagrad,40,20,0.01,55,0.2813737094402313
Adam,33,26,0.01,49,0.1129707545042038
Adadelta,43,28,0.01,68,1.1744383573532104
Adagrad,38,29,0.01,50,0.25323086977005005
Adagrad,34,30,0.01,52,0.26590922474861145
Adagrad,37,19,0.01,70,0.29728245735168457
Adadelta,42,18,0.01,92,1.4688433408737183
Adamax,64,30,0.01,82,0.0689128190279007
Nadam,44,22,0.01,69,0.08962974697351456
Adamax,50,17,0.01,61,0.07715834677219391
AdamW,53,28,0.01,107,0.0894952118396759
Adamax,50,19,0.01,67,0.07242679595947266
