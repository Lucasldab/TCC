Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Best_Value
Adam,51,28,0.01,99,-0.34124553573779093
Adadelta,57,29,0.01,52,-0.3257049550735252
Adamax,63,28,0.01,128,-0.3334222345053227
Adam,39,32,0.01,128,-0.33781843213931667
Ftrl,64,28,0.01,115,-0.3408736201725367
Adagrad,37,30,0.01,58,-0.32823593407220375
Adagrad,64,31,0.01,128,-0.3377341985608737
Nadam,64,25,0.01,128,-0.33378537208365994
Ftrl,44,25,0.01,76,-0.33301525043538266
Adamax,64,32,0.01,100,-0.34746180052495046
Ftrl,57,32,0.01,128,-0.34092050118289835
Adagrad,51,26,0.01,115,-0.33056415522700167
Ftrl,64,17,0.01,128,-0.34239560794533136
AdamW,17,32,0.01,59,-0.32841447682094854
Ftrl,64,31,0.01,99,-0.32756239408639837
Ftrl,59,19,0.01,93,-0.34875781584360765
Adadelta,64,26,0.01,51,-0.3343069311846854
Adagrad,64,22,0.01,117,-0.33229121810164947
Ftrl,60,31,0.01,34,-0.3316410142664767
Adadelta,55,32,0.01,60,-0.3312196499636437
