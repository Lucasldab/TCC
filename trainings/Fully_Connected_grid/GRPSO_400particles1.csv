Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Best_Value
AdamW,56,26,0.01,128,-0.33362726142792226
AdamW,64,32,0.01,105,-0.33305601315548433
RMSprop,64,26,0.01,98,-0.3361671509239473
AdamW,21,25,0.01,85,-0.35117089550480224
Ftrl,64,28,0.01,116,-0.33475353883451336
Adagrad,62,32,0.01,95,-0.331182395715895
Adamax,64,23,0.01,109,-0.35246992586314296
Nadam,61,32,0.01,128,-0.3318542944798316
Adadelta,62,31,0.01,120,-0.33403779474510376
Ftrl,64,27,0.01,96,-0.32774191875712294
Adadelta,46,32,0.01,91,-0.3365647847747547
Ftrl,55,30,0.01,94,-0.33467597504206037
Adagrad,61,8,0.01,88,-0.33540126773049816
Adagrad,62,32,0.01,128,-0.31169737119858365
Ftrl,64,32,0.01,128,-0.331329209210607
Adadelta,64,23,0.01,67,-0.331850922477601
Ftrl,60,18,0.01,100,-0.324173960844175
Nadam,38,29,0.01,97,-0.3336896670439728
Ftrl,52,23,0.01,101,-0.3301900133504544
AdamW,64,26,0.01,117,-0.3350508776006481
