Name,Hidden_Layer1,Hidden_Layer2,Learning_Rate,Batch_Size,Best_Value
AdamW,62,16,0.01,69,-0.3519305077210164
AdamW,54,32,0.01,38,-0.3375440183474446
Adam,52,32,0.01,52,-0.3517768264021828
AdamW,62,28,0.01,75,-0.3487452547278034
RMSprop,50,32,0.01,98,-0.34895652451520687
AdamW,62,21,0.01,128,-0.35057633815281297
Adadelta,49,32,0.01,89,-0.35022135660334686
AdamW,47,23,0.01,89,-0.33939086075606956
Adam,45,32,0.01,120,-0.3455304499363406
RMSprop,58,32,0.01,43,-0.3501852158200799
RMSprop,64,32,0.01,107,-0.3505442287014847
Adam,64,22,0.01,88,-0.3483270289335961
Adam,58,11,0.01,128,-0.3494054742309743
Adam,62,30,0.01,68,-0.34779272091951074
Adagrad,52,31,0.01,67,-0.34761266191734996
Adam,55,27,0.01,51,-0.3492982924446517
AdamW,59,32,0.01,89,-0.3344233070560308
Adam,64,18,0.01,59,-0.33152381238094886
AdamW,61,32,0.01,115,-0.34871211789466866
Adadelta,63,32,0.01,106,-0.3507901483367279
